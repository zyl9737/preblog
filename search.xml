<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pytorch-Day05]]></title>
    <url>%2F2020%2F02%2F19%2FPytorch-Day05%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Pytorch-Day04]]></title>
    <url>%2F2020%2F02%2F19%2FPytorch-Day04%2F</url>
    <content type="text"><![CDATA[机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer 机器翻译用神经网络解决机器翻译问题称为神经机器翻译（NMT）数据预处理：将数据集清洗、转化为神经网络的输入minbatch分词：字符串—-单词组成的列表建立词典：单词组成的列表—-单词id组成的列表载入数据集，得到数据生成器 Encoder-Decoder输出序列的长度可能与源序列的长度不同。Sequence to Sequence模型训练：预测：具体结构： 集束搜索(Beam Search)集束搜索是维特比算法（选择整体分数最高的句子，搜索空间太大）的贪心形式，不能得到全局最优解 注意力机制和Seq2seq模型编码器和解码器本质上是两个RNN，其中编码器对输入序列进行分析编码成一个上下文向量(Context vector)，解码器利用这个编码器生成的向量根据具体任务来进行解码，得到一个新的序列。 编码器如下图就是一个典型的编码器，最终的上下文向量c可以是最后一个时间步的隐藏状态，也可以是编码器每个 解码器下图是两种比较常见的Seq2Seq模型的结构，两个图的左半部分都是上面所说的编码器部分，而右半部分就是解码器部分了。图一是直接将编码器的输出作为解码器的初始隐藏状态，然后直接进行解码。图二是直接将编码器得到的上下文向量输入到解码器的每个时间步中，并且每个时间步的上下文向量是相同，换句话说就是解码器每个时间步都使用了相同的上下文向量。这两种情况可能带来的问题是，当需要编码的句子太长的时候，由于上下文向量能够存储信息的容量是有限的，所以可能会导致信息的丢失，此外，解码器每个时间步的上下文向量都是一个相同的对输入序列的表征，对于上面两种问题，基于注意力机制的Seq2Seq模型给了很好的解决办法。 Attention机制的Seq2Seq在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码，因此， c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。 Attention机制通过在每个时间输入不同的c来解决这个问题，下图是带有Attention机制的Decoder： 每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用$a_{ij}$衡量Encoder中第j阶段的$h_j$和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息$c_i$就来自于所有$h_j$对$a_{ij}$的加权和。以机器翻译为例（将中文翻译成英文）：输入的序列是“我爱中国”，因此，Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的$a_{11}$就比较大，而相应的$a_{12}$、$a_{13}$、$a_{14}$就比较小。c2应该和“爱”最相关，因此对应的$a_{22}$就比较大。最后的c3和h3、h4最相关，因此 $a_{33}$、$a_{34}$的值就比较大。 至此，关于Attention模型，我们就只剩最后一个问题了，那就是：这些权重$a_{ij}$是怎么来的？ 事实上，$a_{ij}$同样是从模型中学出的，它实际和Decoder的第i-1阶段的隐状态、Encoder第j个阶段的隐状态有关。 同样还是拿上面的机器翻译举例，$a_{1j}$的计算（此时箭头就表示对h’和$h_j$同时做变换）： 基于注意力的Seq2Seq模型可以用下图表示： Transformer]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>机器翻译</tag>
        <tag>Seq2seq</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch-Day03]]></title>
    <url>%2F2020%2F02%2F18%2FPytorch-Day03%2F</url>
    <content type="text"><![CDATA[过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶 过拟合、欠拟合及其解决方案几个概念 训练误差（training error）：指在训练集上表现出来的误差 泛化误差（generalizetion error）：指在任意一个测试数据样本上表现出来的误差的期望，并常常通过测试数据集上的误差来近似 验证数据集：预留的在训练数据集和测试数据集以外的数据，用来进行模型选择 K折交叉验证：在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。最后，我们对这K次训练误差和验证误差分别求平均。 欠拟合：指训练误差和泛化误差都不能达到一个较低的水平。 过拟合：指训练误差达到一个较低的水平，而泛化误差依然较大。 产生欠拟合、过拟合的原因主要有两个因素：模型复杂度和序训练数据集大小 当训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。 解决方案过拟合： L2范数正则化（regularization）即权重衰减：L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数。范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。 丢弃法 解决欠拟合可以考虑增加模型复杂度 错题总结测试数据集不可用来调整模型参数，如果使用测试数据集调整模型参数，可能在测试数集上发生一定程度的过拟合现象，此时将不能用测试误差来近似泛化误差。 梯度消失、梯度爆炸模型训练实战步骤顺序： 获取数据集 数据预处理 模型设计 模型验证和模型调整（调参） 模型预测及提交 题目总结梯度消失会导致模型训练困难，对参数的优化步长过小，收效甚微，模型收敛十分缓慢梯度爆炸会导致模型训练困难，对参数的优化步长过大，难以收敛 循环神经网络进阶RNN： GRU RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT） ⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系 重置⻔有助于捕捉时间序列⾥短期的依赖关系 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系 9个权重和偏置参数，外加2个输出层参数和状态初始化（-1）参数初始化参数代码： num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size print(&#39;will use&#39;, device) def get_params(): def _one(shape): ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32) #正态分布 return torch.nn.Parameter(ts, requires_grad=True) def _three(): return (_one((num_inputs, num_hiddens)), _one((num_hiddens, num_hiddens)), torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True)) W_xz, W_hz, b_z = _three() # 更新门参数 W_xr, W_hr, b_r = _three() # 重置门参数 W_xh, W_hh, b_h = _three() # 候选隐藏状态参数 # 输出层参数 W_hq = _one((num_hiddens, num_outputs)) b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True) return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]) def init_gru_state(batch_size, num_hiddens, device): #隐藏状态初始化 return (torch.zeros((batch_size, num_hiddens), device=device), ) LSTM长短期记忆long short-term memory 遗忘门:控制上一时间步的记忆细胞 输入门:控制当前时间步的输入 输出门:控制从记忆细胞到隐藏状态 记忆细胞：⼀种特殊的隐藏状态的信息的流动 深层RNN实现时只需在传统RNN基础上改num_layers参数深层神经网络并非越深越好，层数的加深会导致模型的收敛变得困难，具体选用什么模型还是要看实践效果 双向RNN实现时在传统RNN基础上使bidirectional=True双向循环神经网络在文本任务里能做到同时考虑上下文和当前词之间的依赖前向和后向RNN连结的方式使前面的Ht和后面的Ht用contact进行连结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>拟合</tag>
        <tag>梯度</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch-Day02]]></title>
    <url>%2F2020%2F02%2F13%2FPytorch-Day02%2F</url>
    <content type="text"><![CDATA[《动手学深度学习》Pytorch学习:文本预处理、语言模型、循环神经网络基础 文本预处理包括四个步骤： 读入文本 分词（将一个句子转换为若干个词（token）） 建立字典，将每个词映射到一个唯一的索引 将文本从词的序列转换为索引的序列，方便输入模型 可用现有工具进行分词，spaCy,NLTK 语言模型n元语法n元语法具有以下缺陷： 参数空间过大 数据稀疏 读取数据集with open(&#39;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&#39;) as f: corpus_chars = f.read() print(len(corpus_chars)) print(corpus_chars[: 40]) corpus_chars = corpus_chars.replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;\r&#39;, &#39; &#39;) corpus_chars = corpus_chars[: 10000] 建立字符索引idx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射 char_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射 vocab_size = len(char_to_idx) print(vocab_size) corpus_indices = [char_to_idx[char] for char in corpus_chars] # 将每个字符转化为索引，得到一个索引的序列 sample = corpus_indices[: 20] print(&#39;chars:&#39;, &#39;&#39;.join([idx_to_char[idx] for idx in sample])) print(&#39;indices:&#39;, sample) #定义load_data_jay_lyrices函数 def load_data_jay_lyrics(): with open(&#39;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&#39;) as f: corpus_chars = f.read() corpus_chars = corpus_chars.replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;\r&#39;, &#39; &#39;) corpus_chars = corpus_chars[0:10000] idx_to_char = list(set(corpus_chars)) char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)]) vocab_size = len(char_to_idx) corpus_indices = [char_to_idx[char] for char in corpus_chars] return corpus_indices, char_to_idx, idx_to_char, vocab_size 时序数据的采样有以下两种方法： 随机采样每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。 相邻采样相邻的两个随机小批量在原始序列上的位置相毗邻。 语言模型做错的题 循环神经网络基础做错的题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch-Day01]]></title>
    <url>%2F2020%2F02%2F12%2FPytorch-Day01%2F</url>
    <content type="text"><![CDATA[《动手学深度学习》Pytorch学习:线性回归、Softmax与分类模型、多层感知机 线性回归线性回归模型从零开始的实现# import packages and modules %matplotlib inline import torch from IPython import display from matplotlib import pyplot as plt import numpy as np import random # 生成数据集 # set input feature number num_inputs = 2 # set example number num_examples = 1000 # set true weight and bias in order to generate corresponded label true_w = [2, -3.4] true_b = 4.2 features = torch.randn(num_examples, num_inputs, dtype=torch.float32) labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float32) # 使用图像来展示生成数据 plt.scatter(features[:, 1].numpy(), labels.numpy(), 1); def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # random read 10 samples for i in range(0, num_examples, batch_size): j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch yield features.index_select(0, j), labels.index_select(0, j) batch_size = 10 for X, y in data_iter(batch_size, features, labels): print(X, &#39;\n&#39;, y) break # 初始化模型参数 w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32) b = torch.zeros(1, dtype=torch.float32) w.requires_grad_(requires_grad=True) b.requires_grad_(requires_grad=True) # 定义模型 def linreg(X, w, b): return torch.mm(X, w) + b # 定义损失函数 def squared_loss(y_hat, y): return (y_hat - y.view(y_hat.size())) ** 2 / 2 # 定义优化函数 def sgd(params, lr, batch_size): for param in params: param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track # 训练 # super parameters init lr = 0.03 num_epochs = 5 net = linreg loss = squared_loss # training for epoch in range(num_epochs): # training repeats num_epochs times # in each epoch, all the samples in dataset will be used once # X is the feature and y is the label of a batch sample for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y).sum() # calculate the gradient of batch sample loss l.backward() # using small batch random gradient descent to iter model parameters sgd([w, b], lr, batch_size) # reset parameter gradient w.grad.data.zero_() b.grad.data.zero_() train_l = loss(net(features, w, b), labels) print(&#39;epoch %d, loss %f&#39; % (epoch + 1, train_l.mean().item())) 使用pytorch的简洁实现import torch from torch import nn import numpy as np torch.manual_seed(1) print(torch.__version__) torch.set_default_tensor_type(&#39;torch.FloatTensor&#39;) # 生成数据集 num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float) labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float) # 读取数据集 import torch.utils.data as Data batch_size = 10 # combine featues and labels of dataset dataset = Data.TensorDataset(features, labels) # put dataset into DataLoader data_iter = Data.DataLoader( dataset=dataset, # torch TensorDataset format batch_size=batch_size, # mini batch size shuffle=True, # whether shuffle the data or not num_workers=2, # read data in multithreading ) for X, y in data_iter: print(X, &#39;\n&#39;, y) break # 定义模型 class LinearNet(nn.Module): def __init__(self, n_feature): super(LinearNet, self).__init__() # call father function to init self.linear = nn.Linear(n_feature, 1) # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)` def forward(self, x): y = self.linear(x) return y net = LinearNet(num_inputs) print(net) # ways to init a multilayer network # method one net = nn.Sequential( nn.Linear(num_inputs, 1) # other layers can be added here ) # method two net = nn.Sequential() net.add_module(&#39;linear&#39;, nn.Linear(num_inputs, 1)) # net.add_module ...... # method three from collections import OrderedDict net = nn.Sequential(OrderedDict([ (&#39;linear&#39;, nn.Linear(num_inputs, 1)) # ...... ])) print(net) print(net[0]) # 初始化模型参数 from torch.nn import init init.normal_(net[0].weight, mean=0.0, std=0.01) init.constant_(net[0].bias, val=0.0) # or you can use `net[0].bias.data.fill_(0)` to modify it directly for param in net.parameters(): print(param) # 定义损失函数 loss = nn.MSELoss() # nn built-in squared loss function # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)` # 定义优化函数 import torch.optim as optim optimizer = optim.SGD(net.parameters(), lr=0.03) # built-in random gradient descent function print(optimizer) # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)` # 训练 num_epochs = 3 for epoch in range(1, num_epochs + 1): for X, y in data_iter: output = net(X) l = loss(output, y.view(-1, 1)) optimizer.zero_grad() # reset gradient, equal to net.zero_grad() l.backward() optimizer.step() print(&#39;epoch %d, loss: %f&#39; % (epoch, l.item())) # result comparision dense = net[0] print(true_w, dense.weight.data) print(true_b, dense.bias.data) 做错的两个题 两题都涉及到广播机制，即当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。具体可参考以下两个链接： https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.2_tensor https://pytorch.org/docs/stable/notes/broadcasting.html 关于公式的推导参见以下文章https://mp.weixin.qq.com/s/axFly1Zmw8baifYQF_RnTA 小结视频中用了许多pytorch的函数，由于不是太了解pytorch内的函数，因此查询记录了一下。torch.ones()/torch.zeros()，与MATLAB的ones/zeros很接近。初始化生成均匀分布torch.rand(sizes, out=None) → Tensor返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义。标准正态分布torch.randn(sizes, out=None) → Tensor返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵torch.mm(a, b)是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵torch.Tensor是一种包含单一数据类型元素的多维矩阵，定义了7种CPU tensor和8种GPU tensor类型。random.shuffle(a)：用于将一个列表中的元素打乱。shuffle() 是不能直接访问的，需要导入 random 模块，然后通过 random 静态对象调用该方法。backward()是pytorch中提供的函数，配套有require_grad：1.所有的tensor都有.requires_grad属性,可以设置这个属性.x = tensor.ones(2,4,requires_grad=True)2.如果想改变这个属性，就调用tensor.requires_grad_()方法： x.requires_grad_(False) 和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型、训练数据、损失函数和优化算法。 既可以用神经网络图表示线性回归，又可以用矢量计算表示该模型。 应该尽可能采用矢量计算，以提升计算效率。 Softmax与分类模型Softmax的基本形式Softmax通过下式将输出值变换成值为正且和为1的概率分布： 交叉熵损失函数我们并不需要预测概率完全等于标签概率，而平方损失过于严格。 获取Fashion-MNIST训练集和读取数据这里用到了torchvision包： torchvision.datasets: 一些加载数据的函数及常用的数据集接口； torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等； torchvision.transforms: 常用的图片变换，例如裁剪、旋转等； torchvision.utils: 其他的一些有用的方法。 以下代码在kesci平台运行 # import needed package %matplotlib inline from IPython import display import matplotlib.pyplot as plt import torch import torchvision import torchvision.transforms as transforms import time import sys sys.path.append(&quot;/home/kesci/input&quot;) import d2lzh1981 as d2l # get datasheet mnist_train = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True, transform=transforms.ToTensor()) mnist_test = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=False, download=True, transform=transforms.ToTensor()) # show result print(type(mnist_train)) print(len(mnist_train), len(mnist_test)) feature, label = mnist_train[0] print(feature.shape, label) # Channel x Height x Width # 如果不做变换输入的数据是图像，我们可以看一下图片的类型参数： mnist_PIL = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True) PIL_feature, label = mnist_PIL[0] print(PIL_feature) mnist_PIL = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True) PIL_feature, label = mnist_PIL[0] print(PIL_feature) def show_fashion_mnist(images, labels): d2l.use_svg_display() # 这里的_表示我们忽略（不使用）的变量 _, figs = plt.subplots(1, len(images), figsize=(12, 12)) for f, img, lbl in zip(figs, images, labels): f.imshow(img.view((28, 28)).numpy()) f.set_title(lbl) f.axes.get_xaxis().set_visible(False) f.axes.get_yaxis().set_visible(False) plt.show() X, y = [], [] for i in range(10): X.append(mnist_train[i][0]) # 将第i个feature加到X中 y.append(mnist_train[i][1]) # 将第i个label加到y中 show_fashion_mnist(X, get_fashion_mnist_labels(y)) X, y = [], [] for i in range(10): X.append(mnist_train[i][0]) # 将第i个feature加到X中 y.append(mnist_train[i][1]) # 将第i个label加到y中 show_fashion_mnist(X, get_fashion_mnist_labels(y)) start = time.time() for X, y in train_iter: continue print(&#39;%.2f sec&#39; % (time.time() - start)) Softmax从零开始实现import torch import torchvision import numpy as np import sys sys.path.append(&quot;/home/kesci/input&quot;) import d2lzh1981 as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;) num_inputs = 784 print(28*28) num_outputs = 10 W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float) b = torch.zeros(num_outputs, dtype=torch.float) # 对多维Tensor按维度操作 X = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(X.sum(dim=0, keepdim=True)) # dim为0，按照相同的列求和，并在结果中保留列特征 print(X.sum(dim=1, keepdim=True)) # dim为1，按照相同的行求和，并在结果中保留行特征 print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征 print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征 def softmax(X): X_exp = X.exp() partition = X_exp.sum(dim=1, keepdim=True) # print(&quot;X size is &quot;, X_exp.size()) # print(&quot;partition size is &quot;, partition, partition.size()) return X_exp / partition # 这里应用了广播机制 X = torch.rand((2, 5)) X_prob = softmax(X) print(X_prob, &#39;\n&#39;, X_prob.sum(dim=1)) # Softmax回归模型 def net(X): return softmax(torch.mm(X.view((-1, num_inputs)), W) + b) # 定义损失函数 y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y = torch.LongTensor([0, 2]) y_hat.gather(1, y.view(-1, 1)) def cross_entropy(y_hat, y): return - torch.log(y_hat.gather(1, y.view(-1, 1))) # 定义准确率 def accuracy(y_hat, y): return (y_hat.argmax(dim=1) == y).float().mean().item() print(accuracy(y_hat, y)) # 训练模型 num_epochs, lr = 5, 0.1 # 本函数已保存在d2lzh_pytorch包中方便以后使用 def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, optimizer=None): for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: y_hat = net(X) l = loss(y_hat, y).sum() # 梯度清零 if optimizer is not None: optimizer.zero_grad() elif params is not None and params[0].grad is not None: for param in params: param.grad.data.zero_() l.backward() if optimizer is None: d2l.sgd(params, lr, batch_size) else: optimizer.step() train_l_sum += l.item() train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item() n += y.shape[0] test_acc = evaluate_accuracy(test_iter, net) print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39; % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc)) train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr) # 模型预测 X, y = iter(test_iter).next() true_labels = d2l.get_fashion_mnist_labels(y.numpy()) pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy()) titles = [true + &#39;\n&#39; + pred for true, pred in zip(true_labels, pred_labels)] d2l.show_fashion_mnist(X[0:9], titles[0:9]) Softmax的简洁实现# 加载各种包或者模块 import torch from torch import nn from torch.nn import init import numpy as np import sys sys.path.append(&quot;/home/kesci/input&quot;) import d2lzh1981 as d2l #初始化参数和获取数据 batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;) #定义网络模型 num_inputs = 784 num_outputs = 10 class LinearNet(nn.Module): def __init__(self, num_inputs, num_outputs): super(LinearNet, self).__init__() self.linear = nn.Linear(num_inputs, num_outputs) def forward(self, x): # x 的形状: (batch, 1, 28, 28) y = self.linear(x.view(x.shape[0], -1)) return y # net = LinearNet(num_inputs, num_outputs) class FlattenLayer(nn.Module): def __init__(self): super(FlattenLayer, self).__init__() def forward(self, x): # x 的形状: (batch, *, *, ...) return x.view(x.shape[0], -1) from collections import OrderedDict net = nn.Sequential( # FlattenLayer(), # LinearNet(num_inputs, num_outputs) OrderedDict([ (&#39;flatten&#39;, FlattenLayer()), (&#39;linear&#39;, nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以 ) # 初始化模型参数 init.normal_(net.linear.weight, mean=0, std=0.01) init.constant_(net.linear.bias, val=0) #定义损失函数 loss = nn.CrossEntropyLoss() # 下面是他的函数原型 # class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;) #定义优化函数 optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 下面是函数原型 # class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) #训练 num_epochs = 5 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer) 多层感知机无论添加多少隐藏层，都等价于单层神经网络。解决方法是引入非线性变换，这个非线性函数就叫做激活函数。激活函数主要有以下三种： ReLU函数 Sigmoid函数，(0,1) tanh函数，(-1,1) 关于激活函数的选择： ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。 用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。 在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。 在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。pytorch实现： import torch from torch import nn from torch.nn import init import numpy as np import sys sys.path.append(&quot;/home/kesci/input&quot;) import d2lzh1981 as d2l # 初始化模型和各个参数 num_inputs, num_outputs, num_hiddens = 784, 10, 256 net = nn.Sequential( d2l.FlattenLayer(), nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.Linear(num_hiddens, num_outputs), ) for params in net.parameters(): init.normal_(params, mean=0, std=0.01) # 训练 batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;) loss = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.5) num_epochs = 5 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>线性回归</tag>
        <tag>Softmax与分类模型</tag>
        <tag>多层感知机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于边缘智能的几篇论文的学习笔记]]></title>
    <url>%2F2020%2F02%2F03%2F%E5%85%B3%E4%BA%8E%E8%BE%B9%E7%BC%98%E6%99%BA%E8%83%BD%E7%9A%84%E5%87%A0%E7%AF%87%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[边缘计算(Edge computing)指的是接近于事物，数据和行动源头处的计算。用更通用的术语来表示即：邻近计算或者接近计算(Proximity Computing)。如果云计算是集中式大数据处理，边缘计算则可以理解为边缘式大数据处理。边缘计算已经逐步在物联网、AR/VR场景以及大数据和人工智能行业有所应用。 什么是边缘计算首先，我们仔细观察Edge Computing这个词，其中的edge是和Cloud Computing中的cloud相对应的概念。目前云计算几乎是所有应用程序的主流解决方案，我们的移动端在大多数场景中仅负责发送请求、接收返回数据、渲染画面等操作。在云计算中，庞大的、来自地理位置各异的移动用户终端的服务请求首先通过有线或无线的方式传入接入网(access network)，再经过主干网传播(backbone)传送给服务所在的数据中心进行处理。在这个过程中，位于云端的数据中心才是真正负责处理用户服务请求的地方，而主干网的传播是相对耗时的，这对于那些对延迟极其敏感的应用程序来说是非常不友好的。例如，超高清视频的下载、在线的超高清视频游戏、在线的AR\VR应用程序、自动驾驶等。 我们想办法降低延迟，有两种途径。第一种，也是最常见的一种，怼硬件怼带宽就完事了。显然，这种方式必然是奢侈的。第二种方法则直接改变了计算方式，也就是：尽可能取消请求和数据在主干网的路由。如何做到这一点呢？直接把计算和处理能力从远在天边的云数据中心下沉到距离用户非常近的接入网不就可以了吗！这正是边缘计算的思路。随着5G的到来，这种计算范式上的转变势在必行。我们知道，5G使用了更高的频带，因此无线信号的覆盖范围将会大大受限，为了做到全面覆盖，需要部署很多的微基站。我们有十足的理由赋予这些微基站一定的计算能力，甚至可以在周围建立小型数据中心，直接处理来自微基站转发来的服务请求。这样就完全杜绝了在主干网上极其耗时的路由开销。 当然，以上仅仅是最理想的情况，目前更多的研究人员推崇的是device-edge-cloud synergy，也就是“云-边-端协同处理”。至于怎么个协同法，具体问题具体分析。但是，至少要依据以下特征来定：端通常是计算受限、电池受限的，边和端相比计算能力更强，但是和云相比则是小巫见大巫。但是用户体验的延迟则是正好相反。举个栗子，如果要在移动端完成一个DNN分类的任务，我们可以将已经训练好的DNN模型进行切分，前一半网络层（假设是计算不密集的）放在边缘服务器上，后一半计算密集型的网络层放到云数据中心。用户作为端将待分类的图片发送给边，边将前半部分返回的结果发送给云，由云来完成后半部分，最后的结果再回传给端。这个过程需要在计算开销和通信开销之间做权衡。（注：该案例来自论文Edge Intelligence: On-demand Deep Learning Model Co-inference with Device-edge Synergy） 我们不能将边缘计算剥离出来单独看待。边缘计算更多的是针对物理场景下的考虑，也不是所有的应用程序都需要边缘计算。但是，计算下沉、边端赋能一定是必然会发生的未来。最后，关于边缘计算、雾计算、Cloudlet之间的关系可以参考下面这一页slide: 边缘计算和雾计算的区别随着边缘计算的兴起，理解边缘设备所涉及的另一项技术也很重要，它就是雾计算。 边缘计算具体是指在网络的”边缘”处或附近进行的计算过程，而雾计算则是指边缘设备和云端之间的网络连接。 虽然边缘计算给云计算带来补充，并且与雾计算一起非常紧密地运作，但它绝不是二者的替代者。 如何开展边缘计算的研究优化与设计自底向上可以划分为：Topology、Content、Service。顾名思义，Topology研究的就是边缘计算的架构，这涉及边缘站点的放置及部署(edge site orchestration)、无线网络规划和路由(network planning)等。此处的研究工作与radio access network方向有很大重叠(灌水的快乐源泉:-D)。Content是建立在Topology基础上的：既然我们已经搭建好了边缘网络，接下来要做的就是内容(数据+服务)的部署和放置。这就会涉及到许多问题。例如，要采用哪一种服务架构？以微服务器架构为例，我们需要为各类服务选择合适的边缘站点部署实例，这就得面临服务器选择(server selectioin)、服务放置/部署(service placement/deployment)等问题。如果待部署的服务有复杂的组合结构，那么也会涉及到服务组合的问题(service selection for compositin)等。Content与服务计算关联密切。Service是建立在物理架构和内容放置的基础上的，研究的是如何合理调度资源以提供更加优秀的服务质量何用户体验。包括计算卸载方案的设计(coputing offloading)、用户数据在不同边缘站点之间的同步与迁移(synchronization &amp; migration)、移动性管理(mobility management)等。 以上内容并非是割裂开来的。实际上，许多工作针对以上目标组合式地建模，求解。以上述感兴趣的名词+edge computing作为关键词在谷歌上检索就可以找到许多工作，从模仿开始。对于一个子方向而言，所需要的研究基础差异不大，多读几篇就可以大致建立基本的框架和观念。这类工作其实是非常容易灌水的，因而催生了一大批褒贬不一的、同质的论文。 边缘计算与人工智能结合，也即是边缘智能(Edge Itelligence)，是目前学术界的热点研究方向。这部分的工作自顶向下可分类为模型适配(model adaptation)、框架设计(framework design)和硬件加速(processor acceleration)。边缘智能主要研究如何将人工智能模型(统计学习、深度学习及强化学习均有包含)放在网络边缘端执行。以深度神经网络为例，联邦学习就是一个极佳的训练框架，可以尝试性地放置在资源受限地终端及边缘设备上执行。基于联邦学习这一分布式训练框架，许多工作研究如何通过模型压缩等手段降低DNN模型的资源占用，让其适配计算资源相对受限的网络边缘。 系统与工程这一部分的研究通常与实际问题紧密结合。例如视频监控的实时处理（目标检测、嫌疑行为识别等）、车联网和自动驾驶中的数据实时感知与处理、无人机的实时路径规划与农业灌溉等。即使是研究资源调度和优化的工作，也与从优化角度入手的论文完全不同。这些工作通常借助一些单片机作为边缘设备，借助Docket和Kubernetes搭建benchmarks，并开发相应的中间件来解决这些实际问题。和优化类的工作相比，这类工作研究周期长、设计到要解决的问题所在领域的专业知识，不容易灌水。但是，在理论和模型上难有创新。因此，该问题中的研究基础不好界定。目前SEC（ACM/IEEE Symposium on Edge Computing）这几年的论文偏系统的居多，感兴趣的可以从这个会议上找文献。 做研究的步骤分析问题、建立模型、设计算法求解并做实验验证。从模仿起步是很重要的，当自己没有思路的时候，就可以去顶会顶刊上看看别人是怎么做的，有哪些可以学习的技巧。撰写英文学术论文最主要的是要克服自己的恐惧心理，大大方方去面对这件事情。 这里引用袁、刘老师的几句建议： 我们现在要做的就是找问题找方法，一篇论文要看的是针对哪个领域或者方向的什么问题，目前有哪些挑战，有什么解决方案和方法，创新点是什么，能达到什么效果，还有什么待改进和值得研究的方面。阅读文献有套路（1）看摘要应该注意：思考这一整段文字中的三段逻辑，即，该论文针对1）什么背景下的什么问题，2）提出了自己的什么解决方案，3）效果如何，这三部曲。（2）看概述应该注意：这部分表面看，似乎是个故事的铺垫，不痛不痒，甚至你会感觉可有可无，实则不然，恰恰相反，这部分内容非常非常非常的重要，也最不好写，不好写的原因在于，这部分对问题的全局性进行梳理，逻辑要一环扣一环，需要非常严谨，梳理归纳出问题所在，出如同耍把式卖艺的“打场子”阶段，有了场子，后面就是具体表演了。。。 目前正在看的几篇论文资源约束边缘计算系统中的自适应联邦学习（Adaptive Federated Leaning in Resource Constrainen Edge Computing Systems） 摘要：新兴的技术和应用，包括物联网、社交网络、和众包，在网络边缘产生了大量的数据。机器学习模式往往从收集的数据处建立起来，使检测，分类和预测未来的事件。由于带宽、存储和隐私问题，将所有数据发送到一个集中的地点通常是不切实际的。在本文中，我们考虑了从分布在多个边缘节点上的数据中学习模型参数的问题，而不将原始数据发送到集中的地方。我们的重点是在一类通用的机器学习模型上，使用基于梯度的方法进行训练。从理论角度分析了分布式梯度下降的收敛性。在此基础上，我们提出了一种控制算法，该算法确定了局部更新和全局参数聚合之间的最佳权衡，以最小化给定资源预算下的损失函数。通过对真实数据集的广泛实验，在网络原型系统和更大规模的模拟环境中，对所提出的算法的性能进行了评估。实验结果表明，我们提出的方法在不同的机器学习模型和不同的数据分布下，性能接近最优。 这篇文章得到了一个基于梯度下降的联合学习的新的收敛界，它包含了非独立的和识别的两个界(非i.i.d)节点之间的数据分布和两个全局聚合之间的任意数量的本地更新。 利用上述理论收敛边界，提出了一种学习数据分布、系统力学和模型特征的控制算法，并在此基础上对全局聚合频率和实时性进行了动态适应，研究了全局聚合频率对具有资源约束的联邦学习的适应性，以尽量减少固定资源预算下的学习损失。 今后的研究可以集中在如何充分利用分布式学习的异构资源，以及代表深层神经网络某种形式的的非凸损失函数的理论收敛分析。 边缘AI：通过联合学习对移动边缘计算，缓存和通信进行智能化（In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning） 摘要： 近年来，随着移动通信技术的飞速发展，边缘计算理论和技术受到了全球研究人员和工程师的越来越多的关注，它们可以通过网络边缘显着地弥合云的容量和对设备的需求，并且因此可以加速内容交付并提高移动服务的质量。与传统的优化方法相比，在当前的深度学习技术的驱动下，为了给边缘系统带来更多的智能，我们建议将深度强化学习技术和联合学习框架与移动边缘系统相集成，以优化移动边缘计算，缓存和通讯。因此，我们设计了“ In-Edge AI”框架，以便智能地利用设备和边缘节点之间的协作来交换学习参数，以更好地训练和推断模型，从而进行动态系统级优化和应用程序级增强，同时减少不必要的系统通信负载。对“ In-Edge AI”进行了评估，并证明其具有近乎最佳的性能，但学习开销相对较低，而该系统具有认知性并且适用于移动通信系统。最后，我们讨论了一些相关的挑战和机遇，以揭示有前途的“ In-Edge AI”的未来。 相关工作中考虑的不多的问题： 应该以何种形式收集训练数据（无论是分布式还是集中式） 应该在哪里放置和训练强化学习智能体（无论是在UE，边缘节点还是远程云基础架构中） 强化学习智能体的更新过程应如何进行、协作。 训练数据的隐私保护 这篇文章的创新点： 将DRL与联合学习相结合用于MEC系统中通信和计算的智能联合资源管理，讨论利用DRL的方法（特别是深度Q学习）和分布式DRL，以优化边缘缓存和计算。 提出了”边缘AI”框架，以进一步利用”联合学习”在MEC系统中更好地部署智能资源管理 参考链接： https://www.zhihu.com/question/35792003 https://www.zhihu.com/question/319330609]]></content>
      <categories>
        <category>研究</category>
      </categories>
      <tags>
        <tag>MEC</tag>
        <tag>Federated Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm学习笔记]]></title>
    <url>%2F2020%2F02%2F02%2Fnpm%E6%98%AF%E5%B9%B2%E4%BB%80%E4%B9%88%E7%9A%84%2F</url>
    <content type="text"><![CDATA[最近学习前端碰到很多使用npm的情况，却不知道为什么要使用npm，用这篇博客来详细介绍一下npm。 社区程序员自古以来就有社区文化： 社区的意思是：拥有共同职业或兴趣的人们，自发组织在一起，通过分享信息和资源进行合作。虚拟社区的参与者经常会在线讨论相关话题，或访问某些网站。 前端程序员也有社区，世界上最大的前端社区应该就是 GitHub 了。前端通过 GitHub 来 分享源代码（线上代码仓库） 讨论问题（Issue 列表） 收集学习资源和常去的网站 加入社区最大的好处之一是，你可以使用别人贡献的代码，你也可以贡献代码给别人用。 共享代码前端是怎么共享代码的呢？ 在 GitHub 还没有兴起的年代，前端是通过网址来共享代码 比如你想使用 jQuery，那么你点击 jQuery 网站上提供的链接就可以下载 jQuery，放到自己的网站上使用 GItHub 兴起之后，社区中也有人使用 GitHub 的下载功能： 麻烦当一个网站依赖的代码越来越多，程序员发现这是一件很麻烦的事情： 去 jQuery 官网下载 jQuery 去 BootStrap 官网下载 BootStrap 去 Underscore 官网下载 Underscore …… 有些程序员就受不鸟了，一个拥有三大美德的程序员Isaac Z. Schlueter（以下简称 Isaaz）给出一个解决方案：用一个工具把这些代码集中到一起来管理吧！ 这个工具就是他用 JavaScript （运行在 Node.js 上）写的 npm，全称是 Node Package Manager 具体步骤NPM 的思路大概是这样的： 买个服务器作为代码仓库（registry），在里面放所有需要被共享的代码 发邮件通知 jQuery、Bootstrap、Underscore 作者使用 npm publish 把代码提交到 registry 上，分别取名 jquery、bootstrap 和 underscore（注意大小写） 社区里的其他人如果想使用这些代码，就把 jquery、bootstrap 和 underscore 写到 package.json 里，然后运行 npm install ，npm 就会帮他们下载代码 下载完的代码出现在 node_modules 目录里，可以随意使用了。 这些可以被使用的代码被叫做「包」（package），这就是 NPM 名字的由来：Node Package(包) Manager(管理器)。 发展Isaaz 通知 jQuery 作者 John Resig，他会答应吗？这事儿不一定啊，对不对。 只有社区里的人都觉得 「npm 是个宝」的时候，John Resig 才会考虑使用 npm。 那么 npm 是怎么火的呢？ npm 的发展是跟 Node.js 的发展相辅相成的。 Node.js 是由一个在德国工作的美国程序员 Ryan Dahl 写的。他写了 Node.js，但是 Node.js 缺少一个包管理器，于是他和 npm 的作者一拍即合、抱团取暖，最终 Node.js 内置了 npm。 后来的事情大家都知道，Node.js 火了。 随着 Node.js 的火爆，大家开始用 npm 来共享 JS 代码了，于是 jQuery 作者也将 jQuery 发布到 npm 了。 所以现在，你可以使用 npm install jquery 来下载 jQuery 代码。 现在用 npm 来分享代码已经成了前端的标配。 后续Node.js 目前由 Ryan Dahl 当时所在的公司 joyent 继续开发。Ryan Dahl 现在已经去研究 AI 和机器学习了，并且他把 Node.js 的维护权交给了 Isaaz。（我是不是也应该去研究 AI 和机器学习啊教练） 而 Isaaz 维护了一段时间后，辞职了，成立了一个公司专门维护 npm 的 registry，公司名叫做npm 股份有限公司……谁说开源不能赚钱的~ 社区的力量回顾前端的发展是你会发现，都是社区里的某个人，发布了一份代码，最终影响前端几年的走向。比如 jQuery，比如 Node.js，比如 npm。（其实其他语言也是这样的） 所以，社区的力量是巨大的。 如何使用 NPM安装npm 不需要单独安装。在安装 Node 的时候，会连带一起安装 npm 。但是，Node 附带的 npm 可能不是最新版本，最后用下面的命令，更新到最新版本。$ sudo npm install npm@latest -g 如果是 Window 系统使用以下命令即可： npm install npm -g 也就是使用 npm 安装自己。之所以可以这样，是因为 npm 本身与 Node 的其他模块没有区别。 然后，运行下面的命令，查看各种信息。 # 查看 npm 命令列表 $ npm help # 查看各个命令的简单用法 $ npm -l # 查看 npm 的版本 $ npm -v # 查看 npm 的配置 $ npm config list -l 使用npm initnpm init 用来初始化生成一个新的 package.json 文件。它会向用户提问一系列问题，如果你觉得不用修改默认配置，一路回车就可以了。如果使用了 -f（代表force）、-y（代表yes），则跳过提问阶段，直接生成一个新的 package.json 文件。$ npm init -y npm setnpm set 用来设置环境变量 $ npm set init-author-name &#39;Your name&#39; $ npm set init-author-email &#39;Your email&#39; $ npm set init-author-url &#39;http://yourdomain.com&#39; $ npm set init-license &#39;MIT&#39; 上面命令等于为 npm init 设置了默认值，以后执行 npm init 的时候，package.json 的作者姓名、邮件、主页、许可证字段就会自动写入预设的值。这些信息会存放在用户主目录的 ~/.npmrc文件，使得用户不用每个项目都输入。如果某个项目有不同的设置，可以针对该项目运行 npm config。 npm infonpm info 命令可以查看每个模块的具体信息。比如，查看 underscore 模块的信息。$ npm info underscore上面命令返回一个 JavaScript 对象，包含了 underscore 模块的详细信息。这个对象的每个成员，都可以直接从 info 命令查询。 $ npm info underscore description $ npm info underscore homepage $ npm info underscore version npm searchnpm search 命令用于搜索 npm 仓库，它后面可以跟字符串，也可以跟正则表达式。$ npm search &lt;搜索词&gt; npm listnpm list 命令以树形结构列出当前项目安装的所有模块，以及它们依赖的模块。 $ npm list # 加上 global 参数，会列出全局安装的模块 $ npm list -global # npm list 命令也可以列出单个模块 $ npm list underscore npm install使用 npm 安装包的命令格式为：npm install/i &lt;package_name&gt; 本地模式和全局模式npm 在默认情况下会从 http://npmjs.org 搜索或下载包，将包安装到当前目录的 node_modules 子目录下。 如果你熟悉 Ruby 的 gem 或者 Python 的 pip，你会发现 npm 与它们的行为不同，gem 或 pip 总是以全局模式安装，使包可以供所有的程序使用，而 npm 默认会把包安装到当前目录下。这反映了 npm 不同的设计哲学。如果把包安装到全局，可以提供程序的重复利用程度，避免同样的内容的多分副本，但坏处是难以处理不同的版本依赖。如果把包安装到当前目录，或者说本地，则不会有不同程序依赖不同版本的包的冲突问题，同时还减轻了包作者的 API 兼容性压力，但缺陷则是同一个包可能会被安装许多次。 我们在使用 supervisor 的时候使用了npm install -g supervisor命令，就是以全局模式安装 supervisor 。 这里注意一点的就是，supervisor 必须安装到全局，如果你不安装到全局，错误命令会提示你安装到全局。如果不想安装到默认的全局，也可以自己修改全局路径到当前路径 npm config set prefix &quot;路径&quot;安装完以后就可以用 supervisor 来启动服务了。 supervisor 可以帮助你实现这个功能，它会监视你对代码的驱动，并自动重启 Node.js 。 一般来说，全局安装只适用于工具模块，比如 eslint 和 gulp 。关于使用全局模式，多数时候并不是因为许多程序都有可能用到了它，为了减少多重副本而使用全局模式，而是因为本地模式不会注册 PATH 环境变量。 “本地安装”指的是将一个模块下载到当前项目的 node_modules 子目录，然后只有在项目目录之中，才能调用这个模块。 本地模式和全局模式的特点如下： 模式 可通过 require 使用 注册 PATH 本地模式 是 否 全局模式 否 是 # 本地安装 $ npm install &lt;package name&gt; # 全局安装 $ sudo npm install -global &lt;package name&gt; $ sudo npm install -g &lt;package name&gt; npm install 也支持直接输入 Github 代码库地址。 $ npm install git://github.com/package/path.git $ npm install git://github.com/package/path.git#0.1.0 安装之前，npm install 会先检查，node_modules目录之中是否已经存在指定模块。如果存在，就不再重新安装了，即使远程仓库已经有了一个新版本，也是如此。 如果你希望，一个模块不管是否安装过， npm 都要强制重新安装，可以使用 -f 或 —force 参数。 $ npm install &lt;packageName&gt; --force 安装不同版本install 命令总是安装模块的最新版本，如果要安装模块的特定版本，可以在模块名后面加上 @ 和版本号。 $ npm install sax@latest $ npm install sax@0.1.1 $ npm install sax@&quot;&gt;=0.1.0 &lt;0.2.0&quot; install 命令可以使用不同参数，指定所安装的模块属于哪一种性质的依赖关系，即出现在 packages.json 文件的哪一项中。 –save：模块名将被添加到 dependencies，可以简化为参数-S。–save-dev：模块名将被添加到 devDependencies，可以简化为参数-D。 $ npm install sax --save $ npm install node-tap --save-dev # 或者 $ npm install sax -S $ npm install node-tap -D dependencies 依赖这个可以说是我们 npm 核心一项内容，依赖管理，这个对象里面的内容就是我们这个项目所依赖的 js 模块包。下面这段代码表示我们依赖了 markdown-it 这个包，版本是 ^8.1.0 ，代表最小依赖版本是 8.1.0 ，如果这个包有更新，那么当我们使用 npm install 命令的时候，npm 会帮我们下载最新的包。当别人引用我们这个包的时候，包内的依赖包也会被下载下来。 &quot;dependencies&quot;: { &quot;markdown-it&quot;: &quot;^8.1.0&quot; } devDependencies 开发依赖在我们开发的时候会用到的一些包，只是在开发环境中需要用到，但是在别人引用我们包的时候，不会用到这些内容，放在 devDependencies 的包，在别人引用的时候不会被 npm 下载。 &quot;devDependencies&quot;: { &quot;autoprefixer&quot;: &quot;^6.4.0&quot;,0&quot;, &quot;babel-preset-es2015&quot;: &quot;^6.0.0&quot;, &quot;babel-preset-stage-2&quot;: &quot;^6.0.0&quot;, &quot;babel-register&quot;: &quot;^6.0.0&quot;, &quot;webpack&quot;: &quot;^1.13.2&quot;, &quot;webpack-dev-middleware&quot;: &quot;^1.8.3&quot;, &quot;webpack-hot-middleware&quot;: &quot;^2.12.2&quot;, &quot;webpack-merge&quot;: &quot;^0.14.1&quot;, &quot;highlightjs&quot;: &quot;^9.8.0&quot; } 当你有了一个完整的 package.json 文件的时候，就可以让人一眼看出来，这个模块的基本信息，和这个模块所需要依赖的包。我们可以通过 npm install 就可以很方便的下载好这个模块所需要的包。 npm install 默认会安装 dependencies 字段和 devDependencies 字段中的所有模块，如果使用 —production 参数，可以只安装 dependencies 字段的模块。 $ npm install --production # 或者 $ NODE_ENV=production npm install 一旦安装了某个模块，就可以在代码中用 require 命令加载这个模块。 var backbone = require(&#39;backbone&#39;) console.log(backbone.VERSION) npm runnpm 不仅可以用于模块管理，还可以用于执行脚本。package.json 文件有一个 scripts 字段，可以用于指定脚本命令，供 npm 直接调用。package.json { &quot;name&quot;: &quot;myproject&quot;, &quot;devDependencies&quot;: { &quot;jshint&quot;: &quot;latest&quot;, &quot;browserify&quot;: &quot;latest&quot;, &quot;mocha&quot;: &quot;latest&quot; }, &quot;scripts&quot;: { &quot;lint&quot;: &quot;jshint **.js&quot;, &quot;test&quot;: &quot;mocha test/&quot; } } scripts 脚本顾名思义，就是一些脚本代码，可以通过npm run script-key来调用，例如在这个 package.json 的文件夹下使用npm run dev就相当于运行了 node build/dev-server.js 这一段代码。使用 scripts 的目的就是为了把一些要执行的代码合并到一起，使用 npm run 来快速的运行，方便省事。npm run 是 npm run-script 的缩写，一般都使用前者，但是后者可以更好的反应这个命令的本质。 // 脚本 &quot;scripts&quot;: { &quot;dev&quot;: &quot;node build/dev-server.js&quot;, &quot;build&quot;: &quot;node build/build.js&quot;, &quot;docs&quot;: &quot;node build/docs.js&quot;, &quot;build-docs&quot;: &quot;npm run docs &amp; git checkout gh-pages &amp; xcopy /sy dist\\* . &amp; git add . &amp; git commit -m &#39;auto-pages&#39; &amp; git push &amp; git checkout master&quot;, &quot;build-publish&quot;: &quot;rmdir /S /Q lib &amp; npm run build &amp;git add . &amp; git commit -m auto-build &amp; npm version patch &amp; npm publish &amp; git push&quot;, &quot;lint&quot;: &quot;eslint --ext .js,.vue src&quot; } npm run 如果不加任何参数，直接运行，会列出 package.json 里面所有可以执行的脚本命令。npm 内置了两个命令简写，npm test等同于执行 npm run test``，``npm start 等同于执行 npm run start。 &quot;build&quot;: &quot;npm run build-js &amp;&amp; npm run build-css&quot; 上面的写法是先运行 npm run build-js ，然后再运行 npm run build-css ，两个命令中间用 &amp;&amp; 连接。如果希望两个命令同时平行执行，它们中间可以用 &amp; 连接。 写在 scripts 属性中的命令，也可以在 node_modules/.bin 目录中直接写成 bash 脚本。下面是一个 bash 脚本。 #!/bin/bash cd site/main browserify browser/main.js | uglifyjs -mc &gt; static/bundle.js 假定上面的脚本文件名为 build.sh ，并且权限为可执行，就可以在 scripts 属性中引用该文件。 &quot;build-js&quot;: &quot;bin/build.sh&quot; pre- 和 post- 脚本npm run为每条命令提供了pre-和post-两个钩子（hook）。以npm run lint为例，执行这条命令之前，npm 会先查看有没有定义 prelint 和 postlint 两个钩子，如果有的话，就会先执行npm run prelint，然后执行npm run lint，最后执行npm run postlint。 { &quot;name&quot;: &quot;myproject&quot;, &quot;devDependencies&quot;: { &quot;eslint&quot;: &quot;latest&quot; &quot;karma&quot;: &quot;latest&quot; }, &quot;scripts&quot;: { &quot;lint&quot;: &quot;eslint --cache --ext .js --ext .jsx src&quot;, &quot;test&quot;: &quot;karma start --log-leve=error karma.config.js --single-run=true&quot;, &quot;pretest&quot;: &quot;npm run lint&quot;, &quot;posttest&quot;: &quot;echo &#39;Finished running tests&#39;&quot; } } 上面代码是一个 package.json 文件的例子。如果执行 npm test，会按下面的顺序执行相应的命令。 pretesttestposttest如果执行过程出错，就不会执行排在后面的脚本，即如果 prelint 脚本执行出错，就不会接着执行 lint 和 postlint 脚本。 npm binnpm bin 命令显示相对于当前目录的，Node 模块的可执行脚本所在的目录（即 .bin 目录）。 # 项目根目录下执行 $ npm bin ./node_modules/.bin 创建全局链接npm 提供了一个有趣的命令 npm link，它的功能是在本地包和全局包之间创建符号链接。我们说过使用全局模式安装的包不能直接通过 require 使用。但通过 npm link 命令可以打破这一限制。举个例子，我们已经通过 npm install -g express 安装了 express，这时在工程的目录下运行命令： npm link express ./node_modules/express -&gt; /user/local/lib/node_modules/express 我们可以在 node_modules 子目录中发现一个指向安装到全局的包的符号链接。通过这种方法，我们就可以把全局包当做本地包来使用了。 除了将全局的包链接到本地以外，使用 npm link 命令还可以将本地的包链接到全局。使用方法是在包目录（package.json 所在目录）中运行 npm link 命令。如果我们要开发一个包，利用这种方法可以非常方便地在不同的工程间进行测试。 创建包包是在模块基础上更深一步的抽象，Node.js 的包类似于 C/C++ 的函数库或者 Java、.Net 的类库。它将某个独立的功能封装起来，用于发布、更新、依赖管理和版本控制。Node.js 根据 CommonJS 规范实现了包机制，开发了 npm 来解决包的发布和获取需求。Node.js 的包是一个目录，其中包含了一个 JSON 格式的包说明文件 package.json。严格符合 CommonJS 规范的包应该具备以下特征：。package.json 必须在包的顶层目录下；。二进制文件应该在 bin 目录下；。JavaScript 代码应该在 lib 目录下；。文档应该在 doc 目录下；。单元测试应该在 test 目录下。 Node.js 对包的要求并没有这么严格，只要顶层目录下有 package.json，并符合一些规范即可。当然为了提高兼容性，我们还是建议你在制作包的时候，严格遵守 CommonJS 规范。 我们也可以把文件夹封装为一个模块，即所谓的包。包通常是一些模块的集合，在模块的基础上提供了更高层的抽象，相当于提供了一些固定接口的函数库。通过定制 package.json，我们可以创建更复杂，更完善，更符合规范的包用于发布。 Node.js 在调用某个包时，会首先检查包中 packgage.json 文件的 main 字段，将其作为包的接口模块，如果 package.json 或 main 字段不存在，会尝试寻找 index.js 或 index.node 作为包的接口。 package.json 是 CommonJS 规定的用来描述包的文件，完全符合规范的 package.json 文件应该含有以下字段： name: 包的名字，必须是唯一的，由小写英文字母、数字和下划线组成，不能包含空格。description: 包的简要说明。version: 符合语义化版本识别规范的版本字符串。keywords: 关键字数组，通常用于搜索。maintainers: 维护者数组，每个元素要包含 name 、email(可选)、web(可选)字段。contributors: 贡献者数组，格式与 maintainers 相同。包的作者应该是贡献者数组的第一个元素。bugs: 提交 bug 的地址，可以是网址或者电子邮件地址。licenses: 许可证数组，每个元素要包含 type（许可证的名称）和 url（链接到许可证文本的地址）字段。repositories: 仓库托管地址数组，每个元素要包含 type（仓库的类型，如 git）、URL（仓库的地址）和 path（相对于仓库的路径，可选）字段。dependencies: 包的依赖，一个关联数组，由包名称和版本号组成。 包的发布通过使用npm init可以根据交互式回答产生一个符合标准的 package.json。创建一个 index.js 作为包的接口,一个简单的包就制作完成了。在发布前,我们还需要获得一个账号用于今后维护自己的包,使用npm adduser根据提示完成账号的创建。 完成后可以使用 npm whoami检测是否已经取得了账号。 接下来，在 package.json 所在目录下运行 npm publish，稍等片刻就可以完成发布了，打开浏览器，访问 http://search.npmjs.org/ 就可以找到自己刚刚发布的包了。现在我们可以在世界的任意一台计算机上使用 npm install neveryumodule 命令来安装它。 如果你的包将来有更新,只需要在 package.json 文件中修改 version 字段，然后重新使用 npm publish 命令就行了。如果你对已发布的包不满意，可以使用 npm unpublish命令来取消发布。需要说明的是：json 文件不能有注释 参考链接: https://blog.csdn.net/csdn_yudong/article/details/78946708 http://javascript.ruanyifeng.com/nodejs/npm.html https://zhuanlan.zhihu.com/p/24357770]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>npm</tag>
        <tag>社区文化</tag>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ 1163]]></title>
    <url>%2F2020%2F01%2F31%2FPOJ-1163%2F</url>
    <content type="text"><![CDATA[1163 The Triangle&ensp;&ensp;&ensp;&ensp;&ensp;7&ensp;&ensp;&ensp;&ensp;3 8&ensp;&ensp;&ensp;8 1 0&ensp;&ensp;2 7 4 4&ensp;4 5 2 6 5(Figure 1) Figure 1 shows a number triangle. Write a program that calculates the highest sum of numbers passed on a route that starts at the top and ends somewhere on the base. Each step can go either diagonally down to the left or diagonally down to the right. InputYour program is to read from standard input. The first line contains one integer N: the number of rows in the triangle. The following N lines describe the data of the triangle. The number of rows in the triangle is &gt; 1 but &lt;= 100. The numbers in the triangle, all integers, are between 0 and 99. OutputYour program is to write to standard output. The highest sum is written as an integer.Sample Input 573 88 1 02 7 4 44 5 2 6 5Sample Output 30 分析：最近刚学了动态规划，下面对其做个总结： 一个题目能dp的条件 满足无后效性（“未来与过去无关”，严格定义：如果给定某一阶段的状态，则在这一阶段以后的发展不受这阶段以前各段状态的影响。） 满足最优子结构性质（大问题的最优解可由小问题的最优解推出） dp的本质和快的原因：dp的核心思想就是尽量缩小可能解空间，DP自带剪枝，舍弃了一大堆不可能成为最优解的答案，自然就比常规穷举要快了。总之就是大事化小，小事化了。 设计DP算法的三连：我是谁?—设计状态，表示局面。我从哪里来？我要到哪里去？—设计转移 &lt;/font&gt;题解： #include&lt;stdio.h&gt; #define INF 100 using namespace std; void read(int a[][INF],int m) { int i,j; for(i=0;i&lt;m;i++) { for(j=0;j&lt;=i;j++) scanf(&quot;%d&quot;,&amp;a[i][j]); } } int dp(int a[][INF],int m) { int i,j; for(i=m-2;i&gt;=0;i--) { for(j=0;j&lt;=i;j++) { if(a[i+1][j+1]&gt;a[i+1][j]) a[i][j]+=a[i+1][j+1]; else a[i][j]+=a[i+1][j]; } } return a[0][0]; } int main() { int a[INF][INF],m; while(scanf(&quot;%d&quot;,&amp;m)!=EOF) { read(a,m); int c=dp(a,m); printf(&quot;%d\n&quot;,c); } return 0; }]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Everyone has his time zone]]></title>
    <url>%2F2020%2F01%2F29%2FEveryone-has-his-time-zone%2F</url>
    <content type="text"><![CDATA[New York is 3 hour ahead of California,but it does not make Califonia slow.Someone graduated at the age of 22,but waited 5 years before securing a good job!Someone became a CEO at 25,and died at 50.While another become a CEO at 50,and lived to 90 years.Someone is still single,While someone else got married.Abosolutely everyone in world works based on their Time Zone.People around you might seem to go ahead of you,some might seem to behind you.But everyone is running their own RACE,in their own TIME.Don’t envy them or mock them.They are in their TIME ZONE, and you are in yours!Life is aboout waiting for the right moment to act.So,RELAX.You’re not LATE.You’re not EARLY.You are very much ON TIME,and in your TIME ZONE Destiny set up for you.]]></content>
      <tags>
        <tag>一首小诗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肺炎时期的爱情]]></title>
    <url>%2F2020%2F01%2F25%2F%E8%82%BA%E7%82%8E%E6%97%B6%E6%9C%9F%E7%9A%84%E7%88%B1%E6%83%85%2F</url>
    <content type="text"><![CDATA[姐姐我们明天都戴口罩穿他妈的睡衣上街神州大地六亿电子眼总会有一个拍下我们犯罪的证明一张照片存进硬盘大小5.20Mb被写进永不消逝的通报批评福寿与天齐全国人民为我作见证你牵着我过马路的时候我偷偷望你的眼神里有一颗两颗三四颗星星——《肺炎时期的爱情》by: 刻奇人类研究中心_kitsch]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[First article of the new year]]></title>
    <url>%2F2020%2F01%2F22%2FFirst-article-of-the-new-year%2F</url>
    <content type="text"><![CDATA[2020是鼠年，吸鼠霸王！ 2019就这样过去了，总体来说大二上学期应该说是比较颓废的，退了大一加的所有部门和俱乐部什么的（ACM、网安、计工创协、创业部），想要有一些空闲时间偷偷懒，更好地自学，结果却是经常躺在宿舍混吃等死，于是在大二上的最后一个月，我决定还是得做些什么，不能这么腐烂下去了。遂决定加实验室，鼓起勇气发邮件给老师最后成功加入智能网联与先进计算实验室。不巧的是这事还没定下来时正好有个室友的智能车队走了个队友，拉我去接锅。最后机缘巧合加了两实验室。第一个对期刊会议什么的也有了一些期待，里面有一些优秀的人能让我也沉下心来。第二个实验室让我对硬件不再那么抵触，在回家的前几天我也总算不负众望做出了人车跟随的最初式。 回想了一下，发现我是一个不满足的人，我对新的东西是有好奇心的，什么领域我都想接触一下。这也同时造成了我没有一个十分喜欢的东西，做事情总是三分钟热度。经过我多年玩LOL的经验，这样是不行的，最后上王者的一般都是有着几个专精的英雄和位置，全能王是百年难得一见的。 2019刚到来的时候，我发了个说说，说新的一年祝大家越来越好。但这终究只是祝愿。李B说，这个世界会好吗？相信未来吗？我觉得，会好的。我没办法找到我能改变这个世界的切实证据，但是我还是相信。 新的一年希望自己能学到更多的知识，能C为什么要躺呢？]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>新年</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[攻防世界web新手区题解]]></title>
    <url>%2F2019%2F09%2F16%2F%E6%94%BB%E9%98%B2%E4%B8%96%E7%95%8Cweb%E6%96%B0%E6%89%8B%E5%8C%BA%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[题目：view_source这道题没啥好说的，f12即可flag：cyberpeace{e07dcafaeeb31df23b4d661dd4da56f9} 题目：get_post这道题我使用的方法是：旧版本火狐+旧版本的hackbar hackbar勾选Post，load URL内容为：http://111.198.29.45:33495/?a=1 ， post data内容为：b=2，然后点击Execute即可看到flag了 flag：cyberpeace{c4e43c9c9d0f729358dd9417219a9da0} 题目:robots这个题考到了Robots协议，也就是爬虫排除标准，于是肯定有个robots.txt文件，直接构造url访问这个文件，看到了禁止爬取：f1ag_1s_h3re.php这个页面，我们直接访问这个页面于是便得到了flag了 flag：cyberpeace{1b59446bc8e566382e01b0c209b899bd} 题目：backup这道题考察的是备份文件漏洞，产生该类漏洞的方式一般又三个： 编辑器自动备份 版本控制系统备份 开发者主动备份 于是我们知道了备份文件：index.php.bak 下载后便得到flag了 flag：cyberpeace{4376485b1a095581d7fb57b8ab3bb924} 题目：cookie Cookie是当主机访问Web服务器时，由 Web 服务器创建的，将信息存储在用户计算机上的文件。一般网络用户习惯用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 Session 跟踪而存储在用户本地终端上的数据，而这些数据通常会经过加密处理。 浏览器按下F12键打开开发者工具，刷新后，在存储一栏，可看到名为look-here的cookie的值为cookie.php 访问http://111.198.29.45:47911/cookie.php，提示查看http响应包，在网络一栏，可看到访问cookie.php的数据包 点击查看数据包，在消息头内可发现flag flag：cyberpeace{e865c062128d651191621df4662b3573} 题目：disabled_button这个题对于前端工作者来说绝对的简单的不能再简单了，直接删除掉disabled属性，就可以点击了 flag：cyberpeace{2e978e2dde5d8acdd7ff76f1c426bb29} 题目:simple_js这个题真正的密码部分因该是：\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30 先要把这段16进制转换成10进制得到：55,56,54,79,115,69,114,116,107,49,50 然后直接一段python脚本解得flag s=[55,56,54,79,115,69,114,116,107,49,50]for i in s:print(chr(i),end=’’)flag：Cyberpeace{786OsErtk12} 题目：xff_referer直接刷新一下burp截包，然后添加如下两行内容： X-Forwarded-For:123.123.123.123 Referer:https://www.google.com 然后就看到flag了 flag：cyberpeace{63657c0c7f88a39a475f0de726ef109a} 题目：weak_auth打开网页看到标题提示weak auth弱验证，这就没啥好说的了，没看到验证码，burp直接来爆破吧！ 随便输入下用户名和密码,提示要用admin用户登入,然后跳转到了check.php,查看下源代码提示要用字典。 用burpsuite截下登录的数据包,把数据包发送到intruder爆破 设置爆破点为password 加载字典 开始攻击，查看响应包列表，发现密码为123456时，响应包的长度和别的不一样. 于是便得到了flag flag：cyberpeace{04415bd2dac05f0e2cd712bb43c447b2} 题目：webshell这个没啥好说的，菜刀连接上后发现目录下有个flag.txt，打开就看到了flag了 flag：cyberpeace{74fea3cfddba6bfdc6bfba5b38300b08} 题目：command_execution打开网页在标题看到command execution 命令执行，那就没啥好说的了，看看目录下有些啥吧! ping -c 3 3 127.0.0.1 | ls /binbootdevetchomeliblib64mediamntoptprocrootrunrun.shsbinsrvsystmpusrvar习惯性的看看home里有什么 ping -c 3 127.0.0.1 | ls /homeflag.txtping -c 3 3 127.0.0.1 | cat /home/flag.txtcyberpeace{39190fc825ce46b116b6829f0c13d625}于是便得到了flag！ flag：cyberpeace{39190fc825ce46b116b6829f0c13d625} 题目：simple_php这道题在阅读了PHP代码后，发现，要a==0，但a的值又不能为0，因此让a=0+任意非数字字符，而 b=数字就退出， 于是构造：?a=0a&amp;b=12345A便得到完整的flag flag：Cyberpeace{647E37C7627CC3E4019EC69324F66C7C}]]></content>
      <tags>
        <tag>网安</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机漏洞安全相关的概念]]></title>
    <url>%2F2019%2F08%2F08%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%BC%8F%E6%B4%9E%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[最近在学校的网安协会招新群里做了几个CTF题，发现还是挺有意思的（其实我大一上就加入了，不过是潜水怪，纯混子） 算是重拾对网安的兴趣吧。 POCPOC，Proof ofConcept，中文意思是“观点证明”。这个短语会在漏洞报告中使用，漏洞报告中的POC则是一段说明或者一个攻击的样例，使得读者能够确认这个漏洞是真实存在的。 EXPExploit，中文意思是“漏洞利用”。意思是一段对漏洞如何利用的详细说明或者一个演示的漏洞攻击代码，可以使得读者完全了解漏洞的机理以及利用的方法。 VULVUL，Vulnerability的缩写，泛指漏洞。 CVECVE 的英文全称是“Common Vulnerabilities &amp; Exposures”公共漏洞和暴露，例如CVE-2015-0057、CVE-1999-0001等等。CVE就好像是一个字典表，为广泛认同的信息安全漏洞或者已经暴露出来的弱点给出一个公共的名称。如果在一个漏洞报告中指明的一个漏洞，如果有CVE名称，你就可以快速地在任何其它CVE兼容的数据库中找到相应修补的信息，解决安全问题。 可以在https://cve.mitre.org/ 网站根据漏洞的CVE编号搜索该漏洞的介绍。 也可以在中文社区http://www.scap.org.cn/ 上搜索关于漏洞的介绍 0DAY漏洞和0DAY攻击在计算机领域中，零日漏洞或零时差漏洞（英语：Zero-dayexploit）通常是指还没有补丁的安全漏洞，而零日攻击或零时差攻击（英语：Zero-dayattack）则是指利用这种漏洞进行的攻击。提供该漏洞细节或者利用程序的人通常是该漏洞的发现者。零日漏洞的利用程序对网络安全具有巨大威胁，因此零日漏洞不但是黑客的最爱，掌握多少零日漏洞也成为评价黑客技术水平的一个重要参数。零日漏洞及其利用代码不仅对犯罪黑客而言，具有极高的利用价值，一些国家间谍和网军部队，例如美国国家安全局和美国网战司令部也非常重视这些信息。据路透社报告称美国政府是零日漏洞黑市的最大买家。]]></content>
      <tags>
        <tag>漏洞安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[V2Ray服务器搭建科学上网]]></title>
    <url>%2F2019%2F07%2F04%2FV2ray%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[本文从零开始，手把手教你搭建自己的V2ray和SS服务器实现全球互联。史上最详细的小白搭建V2ray和ss教程。内容包括VPS购买，连接VPS，一键搭建V2ray和SS，开启bbr加速，客户端配置。 购买境外VPS服务器首先进入Vultr官网注册：https://www.vultr.com （通过此链接注册充值10美元送50美元）注意：密码首字母需要大写，且长度需要超过10个字符！！！ 再次确认一遍注册邮箱和密码 登录并进入充值界面进行充值 支持信用卡，支付宝，微信支付。创建服务器点击右上方 “+” 号来创建服务器。选择服务器机房 推荐使用东京和新加坡服务器，物理距离近，延迟要低不少。 选择东京服务器可能需要点耐心，因为使用的人比较多，好多IP被墙，没有耐心的可以选择新加坡服务器。 查询服务器是否被墙下面会有讲到。选择服务器操作系统及配置 选择服务器系统，仅推荐使用Debian9 ，使用其他操作系统可能会有一些列问题。 选择完毕点击创建。 服务器规格选择3.5美元的足矣点击我们刚刚创建的服务器 进入查看服务器配置参数复制服务器IP地址服务器端口扫描 使用端口扫描工具扫描我们创建的服务器IP，查看22端口是否开放，如果是关闭的话按照上面教程重新创建服务器，直到“22”端口为开放状态。 然后再删除之前被墙的服务器，Vultr服务器是按小时收费的，所以我们刚刚创建的服务器删除的话是不收费的。 端口扫描工具我使用的是 http://coolaf.com/tool/port 你也可以上百度搜索端口扫描工具即可。 使用Xshell终端连接服务器安装Xshell Xshell 官网下载 安装完成后打开软件，新建会话。填入服务器IP地址 输入一个名称，方便自己以后管理。 填入服务器IP地址。回到Vultr，复制服务器密码填入服务器用户名及密码 点击用户身份验证 填入服务器用户名root（Vultr用户名默认都是root） 粘贴刚才复制的密码 点击“连接”SSH安全警告 首次连接服务器会出现SSH安全警告，点击“接受并保持”即可 搭建V2ray使用一键安装脚本安装V2ray 推荐使用一键安装脚本，一行代码解决所有问题。 复制下面的代码，然后在Xshell 黑色处点击右键粘贴，然后回车安装。（这里不能使用Ctrl+V粘贴）bash &lt;(curl -s -L https://git.io/v2ray.sh) 也可以通过谷歌云安装官方脚本(据说上面的脚本有后门，对安全性要求高的可以选这个)v2ray官网v2ray配置文件生成器输入 “1” 进行安装选择传输协议 没有特殊需要就直接回车，使用默认的TCP协议。选择端口号 输入端口号，这个自己随意，但是未了避免和以后折腾其他的东西冲突，推荐使用1000以上的端口号但是不能超过65535 建议直接输入“10086” （没有特殊意义，单纯为了好记，PS：中国移动记得给我广告费）广告拦截是否开启广告拦截，看自己需要吧，推荐不要开启，开启广告拦截会消耗服务器资源，国外环境要比国内好得多。开启SS 最好是开启，后面使用游戏加速器会用得上。选择SS端口号 SS端口号，还是随意。但是千万不要和上面V2ray的端口号冲突。输入SS连接密码这个没有要求，只要你自己能记得住就行，越简单越好。选择SS加密协议 不多说，推荐使用默认的。搭建完成 接下来就是回车，回车。 然后喝杯咖啡，等待几分钟，出现下面这个界面就表示服务器搭建完成了 开启BBR 如果使用的是Debian9 系统，BBR是自动开启的。 Ubuntu 18.04/18.10开启BBR加速: Step 1：修改系统变量echo &quot;net.core.default_qdisc=fq&quot; &gt;&gt; /etc/sysctl.conf echo &quot;net.ipv4.tcp_congestion_control=bbr&quot; &gt;&gt; /etc/sysctl.conf Step 2:保存生效sysctl -p Step 3：检查BBR是否开启sysctl net.ipv4.tcp_available_congestion_control如果 返回net.ipv4.tcp_available_congestion_control = bbr cubic reno则 开启 成功！ Step 4：检查BBR是否启动成功lsmod | grep bbr如果 返回tcp_bbr 20480 14则 启动 成功 CentOS 7 开启BBR加速 客户端使用 V2ray和SS客户端使用教程 物尽其用这么高配置的服务器只用来搭建梯子服务器？未免也太浪费了吧，接下来教你在梯子服务器的基础上再搭建一个 个人网盘服务器。 使用NextCloud搭建个人网盘服务器]]></content>
      <tags>
        <tag>v2ray</tag>
        <tag>科学上网</tag>
        <tag>vultr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币与区块链入门]]></title>
    <url>%2F2019%2F06%2F30%2F%E6%AF%94%E7%89%B9%E5%B8%81%E4%B8%8E%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[虚拟货币终将迎来春天 区块链入门比特币入门加密货币的本质知乎大佬靠比特币实现财务自由论坛 阮老师写得很通俗易懂了，关于比特币争议也很大，有看好的也有嗤之以鼻的，看了知乎上前段时间的孙宇晨买下巴菲特午餐的讨论，只想说，币圈真乱。我对很多事情都有极大的兴趣，想探索任何未知的东西，我认为好奇心在人的一生中是至关重要的。我不相信人，认为人的行为是不可控的，所以我非常宅。我相信机器，因为机器的结果都能在我的掌控中。]]></content>
      <tags>
        <tag>哎折腾</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next如何在文章摘要展示图片]]></title>
    <url>%2F2019%2F06%2F28%2FHexo-Next%E5%A6%82%E4%BD%95%E5%9C%A8%E6%96%87%E7%AB%A0%E6%91%98%E8%A6%81%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[一般有两种方法 在文章的属性列表中添加photos属性编写的文章属性中photos默认为文章的配图，这是我目前最喜欢的配图方式，但是有一个缺点，它不能自定义裁剪和缩略比，展示的是原图，这相当于如果你的每张配图大小比例不一致将会很影响美观性，目前没有很好的解决方案，所以只好自己裁剪好再引入。 如这篇文章： --- title: Hexo Next如何在文章摘要展示图片 date: 2019-06-28 17:43:44 tags: Hexo categories: Hexo photos: - &quot;xxx&quot; --- 在你的正文中使用&lt;!-- more --&gt;进行截断由于markdown是支持原生html的，所以我们可以在正文引用img来为我们的文章设置摘要配图,在&lt;!-- more --&gt;之前的内容都会展示到摘要中(同时与你主题文件中配置的摘要字数有关).如： --- title: Hexo Next如何在文章摘要展示图片 date: 2019-06-28 17:43:44 tags: Hexo categories: Hexo --- &lt;img src=&quot;XXX&quot; width=50% /&gt; 哇，漂亮的小姐姐(❤ ω ❤) &lt;!--more--&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记及一些杂谈]]></title>
    <url>%2F2019%2F06%2F20%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E5%8F%8A%E4%B8%80%E4%BA%9B%E6%9D%82%E8%B0%88%2F</url>
    <content type="text"><![CDATA[YOLO]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PAT 1035 插入与归并]]></title>
    <url>%2F2019%2F03%2F18%2FPAT-1035-%E6%8F%92%E5%85%A5%E4%B8%8E%E5%BD%92%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[1035 插入与归并（25 分）根据维基百科的定义： 插入排序是迭代算法，逐一获得输入数据，逐步产生有序的输出序列。每步迭代中，算法从输入序列中取出一元素，将之插入有序序列中正确的位置。如此迭代直到全部元素有序。归并排序进行如下迭代操作：首先将原始序列看成 N 个只包含 1 个元素的有序子序列，然后每次迭代归并两个相邻的有序子序列，直到最后只剩下 1 个有序的序列。现给定原始序列和由某排序算法产生的中间序列，请你判断该算法究竟是哪种排序算法？输入格式：输入在第一行给出正整数 N (≤100)；随后一行给出原始序列的 N 个整数；最后一行给出由某排序算法产生的中间序列。这里假设排序的目标序列是升序。数字间以空格分隔。输出格式：首先在第 1 行中输出Insertion Sort表示插入排序、或Merge Sort表示归并排序；然后在第 2 行中输出用该排序算法再迭代一轮的结果序列。题目保证每组测试的结果是唯一的。数字间以空格分隔，且行首尾不得有多余空格。输入样例 1： 10 3 1 2 8 7 5 9 4 6 0 1 2 3 7 8 5 9 4 6 0输出样例 1： Insertion Sort 1 2 3 5 7 8 9 4 6 0输入样例 2： 10 3 1 2 8 7 5 9 4 0 6 1 3 2 8 5 7 4 9 0 6输出样例 2： Merge Sort 1 2 3 8 4 5 7 9 0 6&lt;/font&gt; 思路：先将i指向中间序列中满足从左到右是从小到大顺序的最后一个下标，再将j指向从i+1开始，第一个不满足a[j] == b[j]的下标，如果j顺利到达了下标n，说明是插入排序，再下一次的序列是sort(a, a+i+2);否则说明是归并排序。归并排序就别考虑中间序列了，直接对原来的序列进行模拟归并时候的归并过程，i从0到n/k，每次一段段得sort(a + i k, a + (i + 1) k);最后别忘记还有最后剩余部分的sort(a + n / k * k, a + n);这样是一次归并的过程。直到有一次发现a的顺序和b的顺序相同，则再归并一次，然后退出循环～ 题解： #include&lt;iostream&gt; #include&lt;algorithm&gt; using namespace std; int main() { int N; int a[101], b[101]; // 原始序列a 中间序列b int i, j; cin&gt;&gt;N; for( i=0; i&lt;N; i++ ) cin&gt;&gt;A1[i]; for( i=0; i&lt;N; i++ ) cin&gt;&gt;A2[i]; for( i=0; b[i]&lt;=b[i+1] &amp;&amp; i&lt;N-1; i++ ) ; // i作为有序序列最后一个元素下标退出循环 for( j=++i; a[j]==b[j] &amp;&amp; j&lt;N; j++ ) ; // a b从第一个无序的元素开始，逐一比对 if( j==N ){// 前半部分有序而后半部分未改动可以确定是插入排序 cout&lt;&lt;&quot;Insertion Sort&quot;&lt;&lt;endl; sort( a, a+i+1 ); } else{ cout&lt;&lt;&quot;Merge Sort&quot;&lt;&lt;endl; int k = 1; int flag=1; //用来标记是否归并到 “中间序列” while( flag ) { flag = 0; for( i=0; i&lt;N; i++ ) if( a[i]!=b[i] ) flag = 1; k*=2; for( i=0; i&lt;N/k; i++ ) sort( a+i*k, a+(i+1)*k ); i=k*(N/k); // 对非偶数序列的“尾巴”进行排序 if(i&lt;N-1) sort( a+k*(N/k), a+N ); } } cout&lt;&lt;a[0]; for( i=1; i&lt;N; i++ ) cout&lt;&lt;&quot; &quot;&lt;&lt;a[i]; cout&lt;&lt;endl; return 0; }]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>插入与归并</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PAT 1055 集体照]]></title>
    <url>%2F2019%2F03%2F18%2FPAT-1055-%E9%9B%86%E4%BD%93%E7%85%A7%2F</url>
    <content type="text"><![CDATA[1055 集体照 （25 分)拍集体照时队形很重要，这里对给定的 N 个人 K 排的队形设计排队规则如下：每排人数为 N/K（向下取整），多出来的人全部站在最后一排；后排所有人的个子都不比前排任何人矮；每排中最高者站中间（中间位置为 m/2+1，其中 m 为该排人数，除法向下取整）；每排其他人以中间人为轴，按身高非增序，先右后左交替入队站在中间人的两侧（例如5人身高为190、188、186、175、170，则队形为175、188、190、186、170。这里假设你面对拍照者，所以你的左边是中间人的右边）；若多人身高相同，则按名字的字典序升序排列。这里保证无重名。现给定一组拍照人，请编写程序输出他们的队形。输入格式：每个输入包含 1 个测试用例。每个测试用例第 1 行给出两个正整数 N（≤10^4​总人数）和 K（≤10^4，总排数）。随后 N 行，每行给出一个人的名字（不包含空格、长度不超过 8 个英文字母）和身高（[30, 300] 区间内的整数）。输出格式：输出拍照的队形。即K排人名，其间以空格分隔，行末不得有多余空格。注意：假设你面对拍照者，后排的人输出在上方，前排输出在下方。输入样例：10 3Tom 188Mike 170Eva 168Tim 160Joe 190Ann 168Bob 175Nick 186Amy 160John 159输出样例：Bob Tom Joe NickAnn Mike EvaTim Amy John 分析：拍照的最后一行是输出的第一行，人数是m=n-n/k*(k-1),其他行数均为m=n/k，用结构体+vector将身高降序排列，注意身高相同名字按升序排列，用while循环排列每一行，将每一行的排列结果的姓名储存在ans数组中，最中间一个学生应该排在m/2的下标位置，即ans[m / 2] = stu[t].name；然后排左边一列，ans数组的下标 j 从m/2-1开始，一直往左j–，而对于stu的下标 i，是从t+1开始，每次隔一个人选取（即i = i+2，因为另一些人的名字是给右边的），每次把stu[i]的name赋值给ans[j–]；排右边的队伍同理，ans数组的下标 j 从m/2 + 1开始，一直往右j++，stu的下标 i，从t+2开始，每次隔一个人选取（i = i+2），每次把stu[i]的name赋值给ans[j++]，然后输出当前已经排好的ans数组～每一次排完一列row-1，直到row等于0时退出循环表示已经排列并输出所有的行～ #include&lt;iostream&gt; #include&lt;algorithm&gt; #include&lt;vector&gt; using namespace std; struct node{ string name; int high; }; int cmp(struct node a,struct node b ) { return a.high!=b.high?a.high&gt;b.high:a.name&lt;b.name; } int main() { int n,k,m; cin&gt;&gt;n&gt;&gt;k; vector&lt;node&gt;stu(n); for(int i=0;i&lt;n;i++) cin&gt;&gt;stu[i].name&gt;&gt;stu[i].high; sort(stu.begin(),stu.end(),cmp); int t=0,row=k; while(row) { if(row==k) m=n-n/k*(k-1); else m=n/k; vector&lt;string&gt;ans(m); ans[m/2]=stu[t].name; //处理左边一列 int j=m/2-1; for(int i=t+1;i&lt;t+m;i=i+2) ans[j--]=stu[i].name; //处理右边一列 j=m/2+1; for(int i=t+2;i&lt;t+m;i=i+2) ans[j++]=stu[i].name; cout&lt;&lt;ans[0]; for(int i=1;i&lt;m;i++) cout&lt;&lt;&quot; &quot;&lt;&lt;ans[i]; cout&lt;&lt;endl; t=t+m;row--; } return 0; }]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PAT 1057 数壹零]]></title>
    <url>%2F2019%2F03%2F18%2FPAT-1057-%E6%95%B0%E5%A3%B9%E9%9B%B6%2F</url>
    <content type="text"><![CDATA[1057 数零壹 （20 分) 给定一串长度不超过 10^5的字符串，本题要求你将其中所有英文字母的序号（字母 a-z 对应序号 1-26，不分大小写）相加，得到整数 N，然后再分析一下 N 的二进制表示中有多少 0、多少 1。例如给定字符串 PAT (Basic)，其字母序号之和为：16+1+20+2+1+19+9+3=71，而 71 的二进制是 1000111，即有 3 个 0、4 个 1。输入格式：输入在一行中给出长度不超过 10^5,以回车结束的字符串。输出格式：在一行中先后输出 0 的个数和 1 的个数，其间以空格分隔。输入样例：PAT (Basic)输出样例：3 4&lt;/font&gt; 分析：用getline接收一行字符串，对于字符串的每一位，如果是字母(isalpha)，则将字母转化为大写(toupper)，并累加(s[i] – ‘A’ + 1)算出n，然后将n转化为二进制，对每一位处理，如果是0则cnt0++，如果是1则cnt1++，最后输出cnt0和cnt1的值～～ 题解： #include&lt;iostream&gt; #include&lt;cstring&gt; #include&lt;cctype&gt; using namespace std; int main() { string s; getline(cin,s); int n=0; for(int i=0;i&lt;s.length();i++) { if(isalpha(s[i])) { s[i]=toupper(s[i]); n+=(s[i]-&#39;A&#39;+1); } } int cnt0=0,cnt1=0; while(n!=0) { if(n%2==0) { cnt0++; } else cnt1++; n=n/2; } cout&lt;&lt;cnt0&lt;&lt;&quot; &quot;&lt;&lt;cnt1; return 0; }]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PAT 1054 求平均值]]></title>
    <url>%2F2019%2F03%2F18%2FPAT-1054-%E6%B1%82%E5%B9%B3%E5%9D%87%E5%80%BC%2F</url>
    <content type="text"><![CDATA[1054 求平均值 （20 分) 本题的基本要求非常简单：给定 N 个实数，计算它们的平均值。但复杂的是有些输入数据可能是非法的。一个“合法”的输入是 [−1000,1000] 区间内的实数，并且最多精确到小数点后 2 位。当你计算平均值的时候，不能把那些非法的数据算在内。输入格式：输入第一行给出正整数 N（≤100）。随后一行给出 N 个实数，数字间以一个空格分隔。输出格式：对每个非法输入，在一行中输出 ERROR: X is not a legal number，其中 X 是输入。最后在一行中输出结果：The average of K numbers is Y，其中 K 是合法输入的个数，Y 是它们的平均值，精确到小数点后 2 位。如果平均值无法计算，则用 Undefined 替换 Y。如果 K 为 1，则输出 The average of 1 number is Y。输入样例 1：75 -3.2 aaa 9999 2.3.4 7.123 2.35输出样例 1：ERROR: aaa is not a legal numberERROR: 9999 is not a legal numberERROR: 2.3.4 is not a legal numberERROR: 7.123 is not a legal numberThe average of 3 numbers is 1.38输入样例 2：2aaa -9999输出样例 2：ERROR: aaa is not a legal numberERROR: -9999 is not a legal numberThe average of 0 numbers is Undefined&lt;/font&gt; 分析：使用sscanf和sprintf函数～sscanf() – 从一个字符串中读进与指定格式相符的数据sprintf() – 字符串格式化命令，主要功能是把格式化的数据写入某个字符串中 题解： #include&lt;iostream&gt; #include&lt;cstring&gt; #include&lt;cstdio&gt; using namespace std; int main() { int n,cnt=0; char a[50],b[50]; cin&gt;&gt;n; double temp,sum=0.0; for(int i=0;i&lt;n;i++) { scanf(&quot;%s&quot;,a); sscanf(a,&quot;%lf&quot;,&amp;temp);//从7.123中读入7.123道temp中 sprintf(b,&quot;%.2f&quot;,temp);//把temp变为7.12写入b中 int flag=0; for(int j=0;j&lt;strlen(a);j++) if(a[j]!=b[j])flag=1; if(flag||temp&lt;-1000||temp&gt;1000) { printf(&quot;ERROR: %s is not a legal number\n&quot;,a); continue; } else { sum+=temp; cnt++; } } if(cnt==1) printf(&quot;The average of 1 number is %.2f&quot;,sum); else if(cnt&gt;1) printf(&quot;The average of %d numbers is %.2f&quot;,cnt,sum/cnt); else printf(&quot;The average of 0 numbers is Undefined&quot;); return 0; }]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>字符串处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdowm语法说明]]></title>
    <url>%2F2019%2F03%2F18%2FMarkdown%E8%AF%AD%E6%B3%95%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[写博客什么的超级好用 *斜体* _斜体第二种方法_ **加粗** __加粗的第二种方法__ ___粗斜体___ 两个enter是换行，或者用&lt;/br&gt;标签表示换行 用一行的=或者-表示一级标题和二级标题。如： 一级标题 ======= 二级标题 -------- 也可以在前面加上一到六个#表示标题的1级到6级。如： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 无序列表：在前面加上 * 或者 + 或者 - 然后加个空格： * ABC * DEF * GHI + JKL + MNO + PQR - STU - VWX - YZZ 有序列表：数字+英文句点+空格。如下： 1. 呵呵 2. 哈哈 3. 嘿嘿 4. 哼哼 &amp;lt; // 会显示为”&lt;“ &amp;amp; // 会显示为”&amp;“：在 href 属性里面，必须将 &amp; 转变为 &amp;amp; \. // 为了防止产生&quot;1.&quot;变为有序列表，则可以写成&quot;1\.&quot; * _ // 如果 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 &gt;只在整个段落的第一行最前面加上大于号可以显示引用（此时出现引用形式，并且为斜体）。但是引言内如果要断行，那个空行也必须在前面加上大于号。就像下面写的酱紫： &gt;&gt;区块引言也可以有级别，在前面加上不同数量的大于号即可。比如说这就是一个二级引言。 &gt;&gt;&gt;这是一个三级引言。格式会显示为字体更小了。 C++ 代码块语法高亮:前后使用```,在前面的三点后写上C++。 建立分割线的方法有： * * * ***** - - - ------------------- 超级链接：[超级链接显示的文字](超级链接的网址，可以是绝对路径、相对路径) 也支持HTML格式的超级链接&lt;a href=&quot;https://www.baidu.com/&quot;&gt;百度&lt;/a&gt; 如果要标记一小段行内程序代码，可以用反引号把它包起来（`），像这样： Use the `printf()` function. 插入图片：![图片的替换文字](图片的地址或路径) ![风景区图片](/Snip20160202_227.png) Email邮件： &lt;123456789@qq.com&gt; 锚点：(能够链接到某个一级标题) [想要显示的名称](#锚点的名称) 参考：https://www.jianshu.com/p/191d1e21f7ed]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>标记语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ STL lower_bound & upper_bound]]></title>
    <url>%2F2019%2F01%2F02%2FC-STL-lower-bound-upper-bound%2F</url>
    <content type="text"><![CDATA[background首先，lower_bound和upper_bound是C++ STL中提供的非常实用的函数。其操作对象可以是vector、set以及map。lower_bound返回值一般是&gt;= 给定val的最小指针（iterator）。upper_bound返回值则是 &gt; 给定val的最小指针（iterator）。 vector中的lower_bound &amp; upper_bound// lower_bound/upper_bound example #include &lt;iostream&gt; // std::cout #include &lt;algorithm&gt; // std::lower_bound, std::upper_bound, std::sort #include &lt;vector&gt; // std::vector int main () { int myints[] = {10,20,30,30,20,10,10,20}; std::vector&lt;int&gt; v(myints,myints+8); // 10 20 30 30 20 10 10 20 std::sort (v.begin(), v.end()); // 10 10 10 20 20 20 30 30 std::vector&lt;int&gt;::iterator low,up; low=std::lower_bound (v.begin(), v.end(), 20); // ^ up= std::upper_bound (v.begin(), v.end(), 20); // ^ std::cout &lt;&lt; &quot;lower_bound at position &quot; &lt;&lt; (low- v.begin()) &lt;&lt; &#39;\n&#39;; std::cout &lt;&lt; &quot;upper_bound at position &quot; &lt;&lt; (up - v.begin()) &lt;&lt; &#39;\n&#39;; return 0; } set中的 lower_bound 和 upper_bound// set::lower_bound/upper_bound #include &lt;iostream&gt; #include &lt;set&gt; int main () { std::set&lt;int&gt; myset; std::set&lt;int&gt;::iterator itlow,itup; for (int i=1; i&lt;10; i++) myset.insert(i*10); // 10 20 30 40 50 60 70 80 90 itlow=myset.lower_bound (30); // ^ itup=myset.upper_bound (60); // ^ // 由于set中没有像vector中那样排序的概念，因此itlow - myset.begin()是错误的，itlow重载这类运算符 myset.erase(itlow,itup); // 10 20 70 80 90 // erase 删除时传入两个iterator，同样删除区间是左闭右开 std::cout &lt;&lt; &quot;myset contains:&quot;; for (std::set&lt;int&gt;::iterator it=myset.begin(); it!=myset.end(); ++it) std::cout &lt;&lt; &#39; &#39; &lt;&lt; *it; std::cout &lt;&lt; &#39;\n&#39;; return 0; } map中的lower_bound 和 upper_bound// map::lower_bound/upper_bound #include &lt;iostream&gt; #include &lt;map&gt; int main () { std::map&lt;char,int&gt; mymap; std::map&lt;char,int&gt;::iterator itlow,itup; mymap[&#39;a&#39;]=20; mymap[&#39;b&#39;]=40; mymap[&#39;c&#39;]=60; mymap[&#39;d&#39;]=80; mymap[&#39;e&#39;]=100; itlow=mymap.lower_bound (&#39;b&#39;); // itlow points to b itup=mymap.upper_bound (&#39;d&#39;); // itup points to e (not d!) 同样返回的是&gt;&#39;d&#39;对应的iterator mymap.erase(itlow,itup); // erases [itlow,itup) // print content: for (std::map&lt;char,int&gt;::iterator it=mymap.begin(); it!=mymap.end(); ++it) std::cout &lt;&lt; it-&gt;first &lt;&lt; &quot; =&gt; &quot; &lt;&lt; it-&gt;second &lt;&lt; &#39;\n&#39;; return 0; }]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《慢慢来，一切都来得及》读书笔记]]></title>
    <url>%2F2018%2F12%2F18%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[因为我知道自己每一天都在认真地生活着，因为我正努力一步步朝着梦想迈进，因此每一天过得还算充实和快乐。这样一想我就不再焦虑了。我又问自己，如果给自己两年的时间去学习英语口语，慢慢来，给自己20年的时间去实现梦想，慢慢来，可以吗？答案是可以。我顿时整个人放松下来，当我允许自己慢慢来时，忽然感觉那种轻装上阵，脚踏实地的坚实力量又回到了自己的身上。有朋友问我，你会一直待在上海吗？我会回答不知道，因为我觉得未来是迷茫的，未来的事情也是难以预料的。我对生活一直怀有很多困惑，我觉得正是这些困惑推动着我不断去思考、去努力、去前进。 你问我不知道自己想要什么，怎么办？我会告诉你，那你赶紧去找啊。在20岁出头的年纪，不知道自己想要什么是一件极其正常的事情，也是一件幸运的事情，因为当你有了困惑之后，你才会思考，才会一步步地寻找自己想要的东西。 如果你真的没有发现自己喜欢的事情，那请不要放过任何尝试的机会，你可以接受各种挑战，尝试去做各种事情，不要拿自己太当回事了，丢弃那虚妄的自尊，不要怕出丑不要怕失败，你甚至要允许自己经常失败，给你面对失败的经验，给自己不断重新再来的勇气，你要做的就是积极地尝试，直到找到自己内心真正的热爱，找到自己愿意为之努力的梦想。要给自己时间，让自己慢慢来，给自己面对失败的勇气和对梦想持续的热情，因为最难的事情不是面对失败，而是面对一而再再而三的失败还能永保热情。 人生从来不是规划出来的，而是一步步走出来的。找到自己喜欢的事情，每天做那么一点点，时间一长，你就会看到自己的成长。 对于很多像我一样缺乏独立思考能力，又不懂得借鉴他人历史教训的人，只有亲身经历过，才能知道自己想要什么样的生活，就算不知道自己到底想要的是怎样的生活，也至少能明白自己不想要怎样的生活。 我一直想不明白，怎么有这么多人这样算计人生呢？人生真的不是算计出来的。有人说：“只有一次的生命，需要活得真性情一点。”真性情就是你不要压抑自己的需求，你要听从自己内心的声音，过自己想要的生活。也许我在念书的时候就结婚生孩子呢？就算念完书，年纪大了再结婚生子又怎样呢？就算我一辈子不结婚生孩子又怎样呢？难道就不会幸福吗？你的生活是需要别人对你说“好”你才会觉得好吗？你的安全感是来自符合社会习俗制定的标准吗？你的幸福感是建立在别人对你的生活投以羡慕嫉妒的目光上的吗？真正的强者是能在人生的旅途中蜕变为只对自己心声负责的达人。 那些早早找到自己的人生梦想，遵循天命的人，固然很幸运；但是，那些还没有找到自己应该走的道路的人也不必感到万分痛苦，因为一切都还来得及，你要给自己慢慢来的机会。我们乡下的老人常常告诫年轻人的一句话：饭要一口口地吃，路要一步步地走。 我常常告诫自己说，想做一件事就要立刻行动起来，不然就跟那些徒有羡慕之情却给自己诸多理由毫无行动的人们一个样。 只有一种英雄主义，就是在认清生活真相之后依然热爱生活 梭罗说：“生命并没有价值，除非你选择并赋予它价值。没有哪一个地方有幸福，除非你为自己带来幸福。” “常有人说我坚持得好，其实真正喜欢的事不用‘坚持’，让自己变得健康，真的很容易，不停地跑下去，就不会老。跑步可以沿途欣赏美景，享受运动的快乐，人生就是一场马拉松，谁健康，谁就能跑得更长远！” “做真正喜欢的事情不用坚持。” 所以，找到自己喜欢的事情非常重要。因为喜欢，你不用苦苦坚持，也因为喜欢，你愿意投入时间、精力，长久以往获得成功就是自然而然的事情。这一点同样适用在寻找爱人这件事上。找到自己真正喜欢的人，与之在一起，并不需要费力坚持，太过辛苦经营，只因为你们在一起是喜欢的、快乐的、充实的。在一起的时间越长，爱情如美酒一般变得愈加醇美。 就像山田本一所说的那样，拆分目标的好处在于：一、使得原本看起来有些吓人的大目标变得容易靠近和比较现实了。当你的心里认定这个目标可以实现时，就不会因为害怕失败而放弃你的行动。人做一件事拖延的原因有很多，其中一个就是把目标定得太高，害怕自己无法实现，其实就是恐惧失败。细化目标可以减少或者避免由于害怕失败而产生的拖延。二、细化目标还可以增加信心。当你觉得目标可以实现时、容易成功时，你就会更有信心。不言而喻的是信心对完成任务的作用很大。 吴淡如在《时间管理幸福学》中说道：“只要想到一件事情可以‘一石二鸟’或‘一石三鸟’我们比较容易有‘赚到’的感觉，会因为自己的‘贪恋’而继续下去。” “为一件事情找到多种目的”在工作和学习中都很适用。 做一件事情时，加强它的正面意义，为它多找一些其他目的，不仅能让你快乐地完成这件事，还让你的生活变得积极而高效，充满正能量。 下雨天的时候，一下班我就匆匆回家，刚到楼梯口，嘴里就念着：泡面、泡面、荷包蛋、荷包蛋……（我是个多容易满足的正牌吃货啊）然后“咔嚓”一声开门，蹬掉高跟鞋，用平底锅煮泡面吃，就着外面的雨声，吃着热气腾腾的荷包蛋泡面。下雨的夜晚，抱着锅，吃着泡面，我会感觉很幸福呢！ 孤独要趁好时光。趁着好时光，独自欣赏月升日落，独自面对生活的波澜起伏。孤独是人生的重要伴侣，学会独处，乐在独处的人也许过得才最自由自在。越来越觉得人的一生归根结底是与自己相处，与自己斗争的过程，要与自己的万千情绪相处，与自己的各种欲望斗争，与自己的软弱、惰性、劣势不停地斗争下去，爱恨情仇，贪嗔痴慢全是你自己一个人的。 上周和许久未见的一个朋友见面，他说我变得自信了。我明白我的自信不是来自薪水的增加、消费能力的提高或者工作能力的增强，而是来源于相信自己有进一步完善自己、改变自己的能力，同时能看到自己的局限，做得到改变能够改变的，接受不能改变的；相信自己有爱自己、爱他人的能力；相信自己一个人生活也过得好。 以前我一直在逃离生活，与生活保持着一定的距离，觉得走到哪里，怎样的生活都不是自己想要的。这两年我感觉自己渐渐脚踏实地了，开始贴着生活在好好地过日子，虽然做得还不够好，但是一直在进步。蓦然回首，我一个人走过了那么多时光，也走了很远，从乡村到都市，从荒凉到繁华，从深夜到清晨，从弱小到强大，从艰难到轻松，从痛苦到狂喜，所有的这些我都一个人一一走过，虽然我走得慢，但是我走得很认真很努力，从没有因为害怕而停止。 通过观察和经验，我发现那些稍微准备就去干的人和非要准备充分才去行动的人最大的区别就在于对人生的认知不同。前者认为人生是各种体验的集合，后者认为人生是各种成功的档案。因此前者往往充满活力和冒险精神，充满勇气和自信。注重过程，乐于接受变化和挑战，不惧怕失败，情绪乐观，面对失败也较轻松和正面，觉得至少能收获一份经验。这样的人常常大胆尝试，敢于打破规则，愿意去做许多未知的事情；后者则畏首畏尾，缺乏勇气和自信，全然以目标为导向。害怕变化和挑战，也非常害怕失败，只要一失败简直会要了他的命，压力沉重，甚至陷入无法自拔的沮丧和毁灭之中，这样的人因为很多的不敢为，所以经历的事情也比较少，囿于自身思维中的各种限制，躲在自己认为的安全区中。 李欣频说：“当你匮乏时不会有人把资源给你，只有当你真正丰富了才会给你。”当你真正做到踏踏实实地完善自己、丰富自己，专心做可以提升自己的事情，学习并拥有更高的技能的时候，很多机会就会降临到你身上。 你经受的每一份痛苦都是上天赐予你的一份神秘礼物（其实能这样想还真不容易），我希望你能从所受的每一份痛苦中获得学习、累积和成长。如果你经历的痛苦仅仅是痛苦，无法将痛苦转换成人生的养分，去灌注自己内心的坚强之花，那么你也许一辈子都痛苦脆弱，与坚强无缘，也找不到自己的存在感。 我一边心不甘情不愿地写着方案，一边想到后面还有那么多工作要做，想着明天就要提交工作成果了。重压、愤怒、怨恨、控诉、敌意和挫败等负面情绪一股脑地冲向我，我崩溃了，居然呜呜地哭了起来。意识到眼泪正夺眶而出，我被自己“正在哭”这一事实吓坏了，这虽然不是我第一次因为工作压力大而哭泣，但那是前两年的事情了，现在我毕竟是工作了三年的职场之人，怎么就这样脆弱，不堪一击？这时心中有一个声音响起：“哭能解决问题吗？难道有人逼你这样做吗？现在这个局面是谁造成的？你打算怎么办？” 趋利避害，逃避责任，这是每一个人都会有的正常心理，但是这并不代表它是好东西，相反，这正是导致许多人生活不幸的原因。 是的，我们需要停止抱怨，抱怨只会带来坏处，一点正面积极的好处都没有的，它会分散你的注意力，消耗你的精力，瓦解你的信心，摧毁你的行动力。抱怨还会限制我们思考，阻挡我们有效工作。因为，当我们抱怨的时候就把焦点放在我们不想要的东西上，所谈论的是负面的、出错的事情，而我们把注意力放在什么上头，那个东西就会扩大。我们抱怨的言语会影响我们的思维，进而影响我们的想法和态度，从而给我们的生活带来负面的影响。同时抱怨还会影响我们的人际关系。试想一下，谁愿意跟一个成天抱怨的人在一起共事呢？ Bronnie Ware专门照顾那些临终病人，听到很多人临终前说出他们一生里最后悔的事。她作了一个概括，有5件事是大多数人最后悔的。 我希望当初我有勇气过自己真正想要的生 活，而不是别人希望我过的生活。 我希望当初我没有花这么多精力在工作上。 我希望当初我能有勇气表达我的感受。 我希望当初我能和朋友保持联系。 我希望当初我能让自己活得开心点。 亲自听闻了1000多例病患的临终遗言后，他写下了这本书，其中排在前五位的是： 没做自己想做的事。 没有实现梦想。 做过对不起良心的事。 被感情左右度过一生。 没有尽力帮助过别人。 我想，人生的意义对任何人来说都显得重要。德国哲学家威廉·施密德在自己《幸福》一书中表达了这样的观点：幸福并不是人生的第一要义，意义才是。我们真正要寻找和建立的并不是幸福，而是意义。我想也许当我们找到自己人生的意义时，我们会觉得自己更有价值，更幸福，更能够战胜人生的虚无和幻灭。 我主张积极地看待自己的童年阴影。如果我们过去的经验、受到的教育、家庭环境和社会环境决定了我们的未来，那不是说明我们的人生早早就被安排好了，这听起来多可怕啊？把自己的问题全部归咎于童年阴影是很不负责的，置自己的主观能动性和创造性于何处？如果依照这种理论，你会带着很负面的能量生活，我劝大家不要信奉。 虽然全世界72.8%的人都会得拖延症，虽然拖延症是个可怕的顽疾，但是治疗起来却也很简单：立即行动。连岳在他的专栏里说：“有什么事让你拖得心烦，先做三分钟再说。如果你是个专栏作家，从昨晚开始就在拖一篇文章，那不如现在马上坐下，打开写字板，先写180秒，于是，奇迹发生了。三分钟后你会继续写，直到把文章写完。” 《少有人走的路》中说“推迟满足感，意味着不贪图暂时的安逸，重新设置人生快乐与痛苦的次序：首先，面对问题并感受痛苦；然后，解决问题并享受更大的快乐，这是唯一可行的生活方式” 不要草率地给自己贴标签。如果你并没有连续20几天持续睡不着、晚睡，只是因为诸如失恋、情绪低落、被领导批评、工作不顺等，那么，不要给自己贴上失眠症、晚睡强迫症、抑郁症之类的标签。这并不是一件值得赶时髦的事情，因为一旦贴上标签，你容易躲在这个舒适的标签里，不愿对自己负责，没有改变自己行为的动力和能量。 管理好白天的时间。不要认为很多事情白天干不了、干不好，这是不良的心理暗示。我有个朋友认为白天写不了文章，只有夜深人静的时候才能安心写稿，其实白天一样可以写稿子，只要自己开始动笔写起来，就会投入其中。学会积极地在白天做科学的时间规划，提高白天的工作效率，避免拖拉，将工作任务在白天完成，把晚上留给睡眠。 每年的3月21日是世界睡眠日，在这一天全球性健康睡眠主题公益活动中有一个“多睡一小时”的活动 让我们互道一声晚安 送走这匆匆的一天 值得怀念的请你珍藏 应该忘记的莫再留恋 让我们互道一声晚安 迎接那崭新的明天 把握那美好的前程 珍惜你锦绣的人生 愿你走进甜甜的梦乡 祝你有个宁静的夜晚 晚安 晚安 再说一声 明天见 你在哪一方面花了时间，那一方面就会回馈给你成果，一切的付出都不会白费如流水。学习如是，工作如是，恋爱亦如是。 兴趣真的有那么重要吗？其实不然。兴趣带来的热情只是最初的火种，想要形成燎原之势还需要我们持续不懈地投入，人是因为把一件事情干得越来越好才越来越有兴趣的，不是对什么感兴趣才干得好。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[first blood]]></title>
    <url>%2F2018%2F12%2F11%2Ffirstblood%2F</url>
    <content type="text"><![CDATA[This is my first blog! 不忘初心，假装这里有很多字 嗯，写完了，滚去四级备考了 考试必过！]]></content>
      <tags>
        <tag>试验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试]]></title>
    <url>%2F2018%2F12%2F11%2F%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
